{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CPU-Bound Offline Topic Modeling with Meno\n",
    "\n",
    "This notebook demonstrates how to use Meno in a CPU-bound offline environment with:\n",
    "- Minimal dependencies installation (`meno[minimal,embeddings]`)\n",
    "- Local model usage (offline copy of all-MiniLM-L6-v2)\n",
    "- Automatic topic number detection\n",
    "- Topic labeling without LLM dependencies\n",
    "- Complete HTML reports and visualizations\n",
    "\n",
    "This approach is ideal for:\n",
    "- Air-gapped environments with no internet\n",
    "- Systems with limited GPU capabilities\n",
    "- When you want minimal dependencies\n",
    "- Processing datasets with unknown topic counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports\n",
    "\n",
    "First, let's import the necessary libraries and set up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "# Add parent directory to path to import meno if needed\n",
    "parent_dir = str(Path().resolve().parent)\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.insert(0, parent_dir)\n",
    "\n",
    "# Import meno components\n",
    "from meno import MenoWorkflow\n",
    "from meno.modeling.embeddings import DocumentEmbedding\n",
    "from meno.visualization.lightweight_viz import plot_topic_landscape, plot_topic_heatmap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration\n",
    "\n",
    "Set up paths and configuration for offline, CPU-optimized usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up paths and configuration\n",
    "# Point to your downloaded model directory - update this path for your system\n",
    "LOCAL_MODEL_PATH = os.path.expanduser(\"~/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/24485cc25a8c8b310657ded9e17f6d18d1bdf0ae\")\n",
    "\n",
    "# You can also check if the model exists in the standard HuggingFace cache location\n",
    "# Uncomment this code to automatically find the model in the cache\n",
    "\"\"\"\n",
    "try:\n",
    "    cache_home = os.path.expanduser(\"~/.cache/huggingface/hub\")\n",
    "    model_files_dir = os.path.join(cache_home, \"models--sentence-transformers--all-MiniLM-L6-v2\")\n",
    "    if os.path.exists(model_files_dir):\n",
    "        # Find snapshots directory\n",
    "        snapshots_dir = os.path.join(model_files_dir, \"snapshots\")\n",
    "        if os.path.exists(snapshots_dir):\n",
    "            snapshot_dirs = [d for d in os.listdir(snapshots_dir) \n",
    "                            if os.path.isdir(os.path.join(snapshots_dir, d))]\n",
    "            if snapshot_dirs:\n",
    "                latest_snapshot = sorted(snapshot_dirs)[-1]\n",
    "                LOCAL_MODEL_PATH = os.path.join(snapshots_dir, latest_snapshot)\n",
    "                print(f\"Found model in HuggingFace cache: {LOCAL_MODEL_PATH}\")\n",
    "            else:\n",
    "                print(\"No snapshot directories found\")\n",
    "        else:\n",
    "            print(\"Snapshots directory not found\")\n",
    "    else:\n",
    "        print(\"Model directory not found in cache\")\n",
    "except Exception as e:\n",
    "    print(f\"Error finding local model: {e}\")\n",
    "\"\"\"\n",
    "\n",
    "# Check if the specified path exists\n",
    "if not os.path.exists(LOCAL_MODEL_PATH):\n",
    "    print(f\"WARNING: Model path {LOCAL_MODEL_PATH} does not exist!\")\n",
    "    print(\"Please update the LOCAL_MODEL_PATH to point to your downloaded model.\")\n",
    "else:\n",
    "    print(f\"Using model from: {LOCAL_MODEL_PATH}\")\n",
    "\n",
    "# Create output directory\n",
    "OUTPUT_DIR = Path(\"./cpu_output\")\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "print(f\"Output will be saved to: {OUTPUT_DIR.absolute()}\")\n",
    "\n",
    "# Configure for optimal CPU performance and offline use\n",
    "CPU_CONFIG = {\n",
    "    \"preprocessing\": {\n",
    "        \"normalization\": {\n",
    "            \"lowercase\": True,\n",
    "            \"remove_punctuation\": True,\n",
    "            \"lemmatize\": True,\n",
    "            \"language\": \"en\",\n",
    "        },\n",
    "        \"stopwords\": {\n",
    "            \"use_default\": True,\n",
    "        },\n",
    "    },\n",
    "    \"modeling\": {\n",
    "        \"embeddings\": {\n",
    "            # CPU optimizations\n",
    "            \"device\": \"cpu\",\n",
    "            \"use_gpu\": False,\n",
    "            \"batch_size\": 32,  # Adjust based on available RAM\n",
    "            \"quantize\": True,  # Memory-efficient model loading\n",
    "            \n",
    "            # Offline settings\n",
    "            \"local_model_path\": LOCAL_MODEL_PATH,\n",
    "            \"local_files_only\": True,\n",
    "        },\n",
    "        \"topic_detection\": {\n",
    "            \"min_topic_size\": 5,  # Adjust based on your data\n",
    "            \"auto_detect_topics\": True,  # Enable automatic topic number detection\n",
    "        },\n",
    "    },\n",
    "    \"visualization\": {\n",
    "        \"plots\": {\n",
    "            \"width\": 900,\n",
    "            \"height\": 600,\n",
    "        },\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Generate Sample Data\n",
    "\n",
    "For this example, we'll generate synthetic data with clear topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data generation function\n",
    "def generate_sample_data(n_samples=200):\n",
    "    \"\"\"Generate synthetic data for demonstration.\"\"\"\n",
    "    print(f\"Generating {n_samples} sample documents...\")\n",
    "    \n",
    "    # Create topic templates\n",
    "    topics = {\n",
    "        \"Technology\": [\n",
    "            \"artificial intelligence machine learning data algorithms computers\",\n",
    "            \"software development programming code application web system\",\n",
    "            \"cloud computing storage server infrastructure network security\",\n",
    "            \"mobile devices apps smartphones tablets technology hardware\"\n",
    "        ],\n",
    "        \"Healthcare\": [\n",
    "            \"medical health doctors patients hospital treatment therapy\",\n",
    "            \"disease diagnosis symptoms medication prescription clinical\",\n",
    "            \"healthcare insurance coverage benefits claims provider\",\n",
    "            \"wellness prevention fitness nutrition exercise lifestyle\"\n",
    "        ],\n",
    "        \"Finance\": [\n",
    "            \"investment market stocks bonds trading portfolio assets\",\n",
    "            \"banking financial loans credit mortgage debt interest rates\",\n",
    "            \"retirement savings pension fund planning wealth management\",\n",
    "            \"insurance risk coverage policy premium claims benefits\"\n",
    "        ],\n",
    "        \"Education\": [\n",
    "            \"learning teaching students school curriculum education classroom\",\n",
    "            \"academic university college degree research scholarship campus\",\n",
    "            \"training skills development career professional certification\",\n",
    "            \"online courses e-learning digital education virtual classroom\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    # Generate documents from topics\n",
    "    documents = []\n",
    "    doc_ids = []\n",
    "    doc_topics = []\n",
    "    \n",
    "    topic_names = list(topics.keys())\n",
    "    doc_id = 1\n",
    "    \n",
    "    for _ in range(n_samples):\n",
    "        # Select a random topic\n",
    "        topic = np.random.choice(topic_names)\n",
    "        doc_topics.append(topic)\n",
    "        \n",
    "        # Select a random template\n",
    "        template = np.random.choice(topics[topic])\n",
    "        \n",
    "        # Create variations by adding noise and varying length\n",
    "        words = template.split()\n",
    "        \n",
    "        # Add some noise and vary length\n",
    "        num_words = len(words) + np.random.randint(-3, 10)\n",
    "        if num_words < 5:\n",
    "            num_words = 5\n",
    "            \n",
    "        # Select random words with replacement to create variations\n",
    "        selected_words = np.random.choice(words, size=num_words, replace=True)\n",
    "        \n",
    "        # Add some random transitional words\n",
    "        transitions = [\"and\", \"also\", \"including\", \"with\", \"for\", \"about\", \"regarding\"]\n",
    "        for i in range(2, len(selected_words), 5):\n",
    "            if i < len(selected_words):\n",
    "                selected_words[i] = np.random.choice(transitions)\n",
    "        \n",
    "        document = \" \".join(selected_words)\n",
    "        documents.append(document)\n",
    "        doc_ids.append(f\"doc_{doc_id}\")\n",
    "        doc_id += 1\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        \"text\": documents,\n",
    "        \"id\": doc_ids,\n",
    "        \"actual_topic\": doc_topics\n",
    "    })\n",
    "    \n",
    "    print(f\"Generated {len(df)} documents across {len(topic_names)} topics\")\n",
    "    return df\n",
    "\n",
    "# Generate the sample data\n",
    "df = generate_sample_data(n_samples=200)\n",
    "\n",
    "# Display a few sample documents\n",
    "print(\"\\nSample documents:\")\n",
    "for topic in df[\"actual_topic\"].unique():\n",
    "    sample = df[df[\"actual_topic\"] == topic].sample(1)[\"text\"].values[0]\n",
    "    print(f\"\\n{topic}: {sample}\")\n",
    "\n",
    "# Save the data for reference\n",
    "df.to_csv(OUTPUT_DIR / \"sample_data.csv\", index=False)\n",
    "print(f\"\\nSample data saved to {OUTPUT_DIR / 'sample_data.csv'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Initialize the Workflow\n",
    "\n",
    "Now we'll set up the MenoWorkflow with offline settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the workflow with offline settings\n",
    "print(\"Initializing MenoWorkflow with offline settings...\")\n",
    "start_time = time.time()\n",
    "\n",
    "workflow = MenoWorkflow(\n",
    "    config_overrides=CPU_CONFIG,\n",
    "    local_model_path=LOCAL_MODEL_PATH,\n",
    "    local_files_only=True,\n",
    "    offline_mode=True  # Critical for bypassing internet checks\n",
    ")\n",
    "\n",
    "# Load the data\n",
    "workflow.load_data(data=df, text_column=\"text\", id_column=\"id\")\n",
    "print(f\"Loaded {len(df)} documents into the workflow\")\n",
    "\n",
    "# Measure initialization time\n",
    "init_time = time.time() - start_time\n",
    "print(f\"Initialization completed in {init_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Preprocessing\n",
    "\n",
    "Generate preprocessing reports and process the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start preprocessing timer\n",
    "start_time = time.time()\n",
    "\n",
    "print(\"Generating preprocessing reports...\")\n",
    "\n",
    "# Generate acronym report\n",
    "try:\n",
    "    workflow.generate_acronym_report(\n",
    "        output_path=OUTPUT_DIR / \"acronyms.html\", \n",
    "        open_browser=False\n",
    "    )\n",
    "    print(f\"Acronym report saved to {OUTPUT_DIR / 'acronyms.html'}\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not generate acronym report: {e}\")\n",
    "\n",
    "# Generate misspelling report\n",
    "try:\n",
    "    workflow.generate_misspelling_report(\n",
    "        output_path=OUTPUT_DIR / \"misspellings.html\", \n",
    "        open_browser=False\n",
    "    )\n",
    "    print(f\"Misspelling report saved to {OUTPUT_DIR / 'misspellings.html'}\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not generate misspelling report: {e}\")\n",
    "\n",
    "# Preprocess documents\n",
    "print(\"\\nPreprocessing documents...\")\n",
    "workflow.preprocess_documents()\n",
    "\n",
    "# Get the preprocessed data\n",
    "preprocessed_df = workflow.get_preprocessed_data()\n",
    "print(f\"Preprocessing completed for {len(preprocessed_df)} documents\")\n",
    "\n",
    "# Display sample of preprocessed text\n",
    "print(\"\\nSample of preprocessed text:\")\n",
    "sample_processed = preprocessed_df[[\"text\", \"processed_text\"]].head(3)\n",
    "display(sample_processed)\n",
    "\n",
    "# Measure preprocessing time\n",
    "preproc_time = time.time() - start_time\n",
    "print(f\"Preprocessing completed in {preproc_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Topic Modeling with Automatic Topic Detection\n",
    "\n",
    "Now we'll run topic modeling with automatic topic number detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start topic modeling timer\n",
    "start_time = time.time()\n",
    "\n",
    "print(\"Discovering topics with automatic topic detection...\")\n",
    "workflow.discover_topics(\n",
    "    method=\"embedding_cluster\",\n",
    "    auto_detect_topics=True,\n",
    "    modeling_approach=\"lightweight\",  # Use efficient model like NMF\n",
    ")\n",
    "\n",
    "# Get topic information\n",
    "topics_df = workflow.get_topic_assignments()\n",
    "topic_info = workflow.modeler.get_topic_info()\n",
    "print(f\"\\nDiscovered {len(topic_info)} topics automatically\")\n",
    "\n",
    "# Display topic distribution\n",
    "print(\"\\nTopic distribution:\")\n",
    "display(topic_info[['Topic', 'Size', 'Name']])\n",
    "\n",
    "# Display top words per topic\n",
    "print(\"\\nTop words per topic:\")\n",
    "for _, row in topic_info.iterrows():\n",
    "    topic_id = row[\"Topic\"]\n",
    "    topic_words = workflow.modeler.get_topic_words(topic_id, top_n=10)\n",
    "    word_str = \", \".join([word for word, _ in topic_words[:10]])\n",
    "    print(f\"Topic {topic_id} ({row['Size']} docs): {word_str}\")\n",
    "\n",
    "# Compare with ground truth\n",
    "if \"actual_topic\" in df.columns:\n",
    "    # Get document assignment with original IDs\n",
    "    doc_topics = topics_df.merge(df[[\"id\", \"actual_topic\"]], on=\"id\")\n",
    "    \n",
    "    # Show contingency table\n",
    "    print(\"\\nContingency table (Discovered vs. Actual):\")\n",
    "    contingency = pd.crosstab(doc_topics[\"topic\"], doc_topics[\"actual_topic\"])\n",
    "    display(contingency)\n",
    "    \n",
    "    # Calculate adjusted mutual information\n",
    "    from sklearn.metrics import adjusted_mutual_info_score\n",
    "    ami = adjusted_mutual_info_score(doc_topics[\"topic\"], doc_topics[\"actual_topic\"])\n",
    "    print(f\"\\nAdjusted Mutual Information: {ami:.4f}\")\n",
    "\n",
    "# Measure topic modeling time\n",
    "topic_time = time.time() - start_time\n",
    "print(f\"\\nTopic modeling completed in {topic_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Auto-Label Topics (No LLM Required)\n",
    "\n",
    "Create meaningful topic labels from keywords without using an LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auto-label topics using keywords\n",
    "print(\"Generating topic labels from keywords...\")\n",
    "topic_labels = {}\n",
    "\n",
    "for topic_id in topic_info[\"Topic\"].unique():\n",
    "    if topic_id == -1:\n",
    "        topic_labels[topic_id] = \"Outliers\"\n",
    "        continue\n",
    "        \n",
    "    # Get top words for this topic\n",
    "    top_words = workflow.modeler.get_topic_words(topic_id, top_n=5)\n",
    "    keywords = [word for word, _ in top_words]\n",
    "    \n",
    "    # Create a descriptive label from top words\n",
    "    label = \" & \".join(keywords[:2]) + \" Topic\"\n",
    "    topic_labels[topic_id] = label\n",
    "\n",
    "# Print auto-generated labels\n",
    "print(\"\\nAuto-generated topic labels:\")\n",
    "for topic_id, label in topic_labels.items():\n",
    "    topic_size = topic_info[topic_info[\"Topic\"] == topic_id][\"Size\"].values[0]\n",
    "    print(f\"Topic {topic_id} ({topic_size} docs): {label}\")\n",
    "\n",
    "# Save topic information with labels\n",
    "topic_info[\"Label\"] = topic_info[\"Topic\"].map(topic_labels)\n",
    "topic_info.to_csv(OUTPUT_DIR / \"topic_summary.csv\", index=False)\n",
    "print(f\"\\nTopic summary saved to {OUTPUT_DIR / 'topic_summary.csv'}\")\n",
    "\n",
    "# Save document-topic assignments\n",
    "topics_df[\"Topic_Label\"] = topics_df[\"topic\"].map(topic_labels)\n",
    "topics_df.to_csv(OUTPUT_DIR / \"document_topics.csv\", index=False)\n",
    "print(f\"Document-topic assignments saved to {OUTPUT_DIR / 'document_topics.csv'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Generate Comprehensive Report\n",
    "\n",
    "Create an interactive HTML report with all the topic modeling results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start report generation timer\n",
    "start_time = time.time()\n",
    "\n",
    "print(\"Generating comprehensive report...\")\n",
    "report_path = workflow.generate_comprehensive_report(\n",
    "    output_path=OUTPUT_DIR / \"topic_report.html\",\n",
    "    open_browser=False,\n",
    "    title=\"CPU-Optimized Topic Analysis Report\"\n",
    ")\n",
    "\n",
    "print(f\"Report generated at {report_path}\")\n",
    "\n",
    "# Measure report generation time\n",
    "report_time = time.time() - start_time\n",
    "print(f\"Report generation completed in {report_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Additional Visualizations\n",
    "\n",
    "Create additional CPU-efficient visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start visualization timer\n",
    "start_time = time.time()\n",
    "\n",
    "print(\"Creating additional visualizations...\")\n",
    "\n",
    "# Topic distribution\n",
    "try:\n",
    "    dist_fig = workflow.modeler.visualize_topic_distribution(return_figure=True)\n",
    "    dist_fig.write_html(OUTPUT_DIR / \"topic_distribution.html\")\n",
    "    print(f\"Topic distribution saved to {OUTPUT_DIR / 'topic_distribution.html'}\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not create topic distribution: {e}\")\n",
    "\n",
    "# Embedding visualization\n",
    "try:\n",
    "    embed_fig = workflow.modeler.visualize_embeddings(return_figure=True)\n",
    "    embed_fig.write_html(OUTPUT_DIR / \"topic_embeddings.html\")\n",
    "    print(f\"Topic embeddings saved to {OUTPUT_DIR / 'topic_embeddings.html'}\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not create embedding visualization: {e}\")\n",
    "\n",
    "# Get model and documents for specialized visualizations\n",
    "model = workflow.modeler.topic_model\n",
    "documents = workflow.get_preprocessed_data()[\"processed_text\"].tolist()\n",
    "\n",
    "# Create PCA-based visualization (CPU-efficient alternative to UMAP)\n",
    "try:\n",
    "    print(\"\\nGenerating PCA-based topic landscape...\")\n",
    "    landscape = plot_topic_landscape(\n",
    "        model=model,\n",
    "        documents=documents,\n",
    "        method=\"pca\",  # PCA is more CPU-efficient than UMAP\n",
    "        width=900,\n",
    "        height=600\n",
    "    )\n",
    "    landscape.write_html(OUTPUT_DIR / \"topic_landscape_pca.html\")\n",
    "    print(f\"PCA topic landscape saved to {OUTPUT_DIR / 'topic_landscape_pca.html'}\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not create topic landscape: {e}\")\n",
    "\n",
    "# Topic heatmap (if multiple models were trained)\n",
    "try:\n",
    "    print(\"\\nGenerating topic heatmap...\")\n",
    "    heatmap = plot_topic_heatmap(\n",
    "        model=model,\n",
    "        documents=documents,\n",
    "        width=900,\n",
    "        height=600\n",
    "    )\n",
    "    heatmap.write_html(OUTPUT_DIR / \"topic_heatmap.html\")\n",
    "    print(f\"Topic heatmap saved to {OUTPUT_DIR / 'topic_heatmap.html'}\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not create topic heatmap: {e}\")\n",
    "\n",
    "# Measure visualization time\n",
    "viz_time = time.time() - start_time\n",
    "print(f\"\\nVisualizations completed in {viz_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Performance Summary\n",
    "\n",
    "Summarize the performance metrics of our CPU-optimized workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create performance summary\n",
    "print(\"Performance Summary\")\n",
    "print(\"===================\\n\")\n",
    "print(f\"Dataset size: {len(df)} documents\")\n",
    "print(f\"Topics discovered: {len(topic_info)}\")\n",
    "print(\"\\nProcessing times:\")\n",
    "print(f\"- Initialization: {init_time:.2f} seconds\")\n",
    "print(f\"- Preprocessing: {preproc_time:.2f} seconds\")\n",
    "print(f\"- Topic modeling: {topic_time:.2f} seconds\")\n",
    "print(f\"- Report generation: {report_time:.2f} seconds\")\n",
    "print(f\"- Visualizations: {viz_time:.2f} seconds\")\n",
    "print(f\"- Total processing time: {init_time + preproc_time + topic_time + report_time + viz_time:.2f} seconds\")\n",
    "print(f\"- Documents per second: {len(df)/(init_time + preproc_time + topic_time):.2f}\")\n",
    "\n",
    "if \"actual_topic\" in df.columns:\n",
    "    print(f\"\\nAdjusted Mutual Information: {ami:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Summary and Conclusion\n",
    "\n",
    "This notebook demonstrated a complete CPU-optimized workflow for topic modeling in an offline environment using Meno. We were able to:\n",
    "\n",
    "1. Use local models without internet connectivity\n",
    "2. Configure for optimal CPU performance\n",
    "3. Automatically detect the optimal number of topics\n",
    "4. Generate meaningful topic labels without LLMs\n",
    "5. Create comprehensive reports and visualizations\n",
    "\n",
    "All outputs have been saved to the `cpu_output` directory for further analysis.\n",
    "\n",
    "### Key Benefits of this Approach\n",
    "\n",
    "- Works completely offline\n",
    "- Requires minimal dependencies (just `meno[minimal,embeddings]`)\n",
    "- Efficiently uses CPU resources\n",
    "- Provides high-quality topic detection with auto-determined topic counts\n",
    "- Creates meaningful topic labels from document content\n",
    "- Generates interactive visualizations optimized for performance"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}