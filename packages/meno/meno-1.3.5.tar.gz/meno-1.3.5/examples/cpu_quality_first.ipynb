{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quality-First CPU Topic Modeling with Meno\n",
    "\n",
    "This notebook demonstrates how to prioritize quality over speed when running Meno on CPU-bound systems. This approach is ideal when:\n",
    "\n",
    "- Processing time is not a critical concern\n",
    "- You want the highest quality results possible\n",
    "- You don't have GPU acceleration available\n",
    "- You need superior topic separation and visualization\n",
    "\n",
    "We'll use:\n",
    "- The full-featured `all-MiniLM-L6-v2` embedding model\n",
    "- UMAP dimensionality reduction (slower but better quality than PCA)\n",
    "- BERTopic with HDBSCAN clustering for optimal topic coherence\n",
    "- Detailed visualizations optimized for quality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports\n",
    "\n",
    "First, let's import the necessary libraries and set up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add parent directory to path to import meno if needed\n",
    "parent_dir = str(Path().resolve().parent)\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.insert(0, parent_dir)\n",
    "\n",
    "# Import meno components\n",
    "from meno import MenoWorkflow, MenoTopicModeler\n",
    "from meno.modeling.embeddings import DocumentEmbedding\n",
    "from meno.modeling.bertopic_model import BERTopicModel\n",
    "from meno.visualization.bertopic_viz import create_bertopic_hierarchy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration\n",
    "\n",
    "Set up paths and configuration optimized for quality results."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Set up paths and configuration\n# Point to your downloaded model directory - update this path for your system\nLOCAL_MODEL_PATH = os.path.expanduser(\"~/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/24485cc25a8c8b310657ded9e17f6d18d1bdf0ae\")\n\n# You can also check if the model exists in the standard HuggingFace cache location\n# Uncomment this code to automatically find the model in the cache\n\"\"\"\ntry:\n    cache_home = os.path.expanduser(\"~/.cache/huggingface/hub\")\n    model_files_dir = os.path.join(cache_home, \"models--sentence-transformers--all-MiniLM-L6-v2\")\n    if os.path.exists(model_files_dir):\n        # Find snapshots directory\n        snapshots_dir = os.path.join(model_files_dir, \"snapshots\")\n        if os.path.exists(snapshots_dir):\n            snapshot_dirs = [d for d in os.listdir(snapshots_dir) \n                            if os.path.isdir(os.path.join(snapshots_dir, d))]\n            if snapshot_dirs:\n                latest_snapshot = sorted(snapshot_dirs)[-1]\n                LOCAL_MODEL_PATH = os.path.join(snapshots_dir, latest_snapshot)\n                print(f\"Found model in HuggingFace cache: {LOCAL_MODEL_PATH}\")\n            else:\n                print(\"No snapshot directories found\")\n        else:\n            print(\"Snapshots directory not found\")\n    else:\n        print(\"Model directory not found in cache\")\nexcept Exception as e:\n    print(f\"Error finding local model: {e}\")\n\"\"\"\n\n# Check if the specified path exists\nif not os.path.exists(LOCAL_MODEL_PATH):\n    print(f\"WARNING: Model path {LOCAL_MODEL_PATH} does not exist!\")\n    print(\"Please update the LOCAL_MODEL_PATH to point to your downloaded model.\")\nelse:\n    print(f\"Using model from: {LOCAL_MODEL_PATH}\")\n\n# Create output directory\nOUTPUT_DIR = Path(\"./quality_output\")\nOUTPUT_DIR.mkdir(exist_ok=True)\nprint(f\"Output will be saved to: {OUTPUT_DIR.absolute()}\")\n\n# Configure for quality-first CPU usage\nQUALITY_CONFIG = {\n    \"preprocessing\": {\n        \"normalization\": {\n            \"lowercase\": True,\n            \"remove_punctuation\": True,\n            \"remove_stopwords\": True,\n            \"lemmatize\": True,\n            \"language\": \"en\",\n        },\n    },\n    \"modeling\": {\n        \"embeddings\": {\n            # Use best embedding model\n            \"model_name\": \"all-MiniLM-L6-v2\",\n            \"local_model_path\": LOCAL_MODEL_PATH,\n            \"local_files_only\": True,\n            \n            # CPU settings (but not optimized for speed)\n            \"device\": \"cpu\",\n            \"use_gpu\": False,\n            \n            # Quality-focused settings\n            \"precision\": \"float32\",  # Full precision for best quality\n            \"quantize\": False,        # No quantization for best quality\n            \"batch_size\": 16,         # Smaller batch size for better memory management\n        },\n        # High-quality HDBSCAN clustering settings\n        \"clustering\": {\n            \"min_cluster_size\": 5,\n            \"min_samples\": 5,\n            \"prediction_data\": True,\n        },\n    },\n    \"visualization\": {\n        # High-quality UMAP settings\n        \"umap\": {\n            \"n_neighbors\": 15,  # Higher for more global structure\n            \"n_components\": 3,   # 3D visualization\n            \"min_dist\": 0.1,\n            \"metric\": \"cosine\",\n            \"low_memory\": False,  # Quality over memory efficiency\n        },\n        \"plots\": {\n            \"width\": 1000,       # Larger plots for detail\n            \"height\": 800,\n            \"template\": \"plotly_white\",\n        },\n    },\n}",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Generate Sample Data\n",
    "\n",
    "For this example, we'll generate synthetic data with subtle topic overlaps to demonstrate quality-focused modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data generation function with more nuanced topics\n",
    "def generate_quality_sample_data(n_samples=300):\n",
    "    \"\"\"Generate synthetic data for demonstration with subtle topic overlaps.\"\"\"\n",
    "    print(f\"Generating {n_samples} sample documents with nuanced topics...\")\n",
    "    \n",
    "    # Create topic templates with some overlapping terms\n",
    "    topics = {\n",
    "        \"AI Technology\": [\n",
    "            \"artificial intelligence neural networks deep learning algorithms training data\",\n",
    "            \"machine learning models prediction classification regression computer vision\",\n",
    "            \"natural language processing transformers bert gpt text generation tokens\",\n",
    "            \"reinforcement learning agents environments rewards optimization policy\"\n",
    "        ],\n",
    "        \"Data Science\": [\n",
    "            \"data analysis statistics regression visualization insights correlation\",\n",
    "            \"big data processing pipelines hadoop spark streaming computation\",\n",
    "            \"predictive modeling machine learning algorithms classification accuracy\",\n",
    "            \"data science projects python pandas numpy visualization matplotlib\"\n",
    "        ],\n",
    "        \"Healthcare Analytics\": [\n",
    "            \"medical data analysis patient outcomes treatment effectiveness metrics\",\n",
    "            \"healthcare analytics prediction hospital readmission prevention care\",\n",
    "            \"clinical decision support systems algorithms evidence patient data\",\n",
    "            \"medical imaging analysis deep learning detection diagnosis pathology\"\n",
    "        ],\n",
    "        \"Financial Technology\": [\n",
    "            \"fintech innovation banking technology digital payments blockchain\",\n",
    "            \"algorithmic trading market prediction financial models risk analysis\",\n",
    "            \"cryptocurrency blockchain transactions distributed ledger smart contracts\",\n",
    "            \"financial data analysis machine learning fraud detection patterns\"\n",
    "        ],\n",
    "        \"Sustainable Energy\": [\n",
    "            \"renewable energy solar wind hydroelectric power generation efficiency\",\n",
    "            \"smart grid optimization data analysis consumption forecasting models\",\n",
    "            \"energy storage technology batteries capacity efficiency innovation\",\n",
    "            \"carbon emissions reduction monitoring data analysis climate impact\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    # Create some cross-topic terms to make distinctions more subtle\n",
    "    cross_topic_terms = {\n",
    "        (\"AI Technology\", \"Data Science\"): \n",
    "            [\"algorithms\", \"machine learning\", \"prediction\", \"models\", \"classification\"],\n",
    "        (\"AI Technology\", \"Healthcare Analytics\"): \n",
    "            [\"medical imaging\", \"diagnosis\", \"prediction\", \"deep learning\"],\n",
    "        (\"Data Science\", \"Financial Technology\"): \n",
    "            [\"data analysis\", \"prediction\", \"models\", \"algorithms\"],\n",
    "        (\"AI Technology\", \"Sustainable Energy\"): \n",
    "            [\"optimization\", \"prediction\", \"models\", \"forecasting\"],\n",
    "        (\"Data Science\", \"Healthcare Analytics\"): \n",
    "            [\"data analysis\", \"prediction\", \"patient data\", \"outcomes\"]\n",
    "    }\n",
    "    \n",
    "    # Generate documents from topics\n",
    "    documents = []\n",
    "    doc_ids = []\n",
    "    doc_topics = []\n",
    "    doc_subtopics = []\n",
    "    \n",
    "    topic_names = list(topics.keys())\n",
    "    doc_id = 1\n",
    "    \n",
    "    for _ in range(n_samples):\n",
    "        # Select a random primary topic\n",
    "        primary_topic = np.random.choice(topic_names)\n",
    "        doc_topics.append(primary_topic)\n",
    "        \n",
    "        # Select a random template\n",
    "        template = np.random.choice(topics[primary_topic])\n",
    "        words = template.split()\n",
    "        \n",
    "        # With some probability, add influence from another topic\n",
    "        if np.random.random() < 0.3:  # 30% chance of topic overlap\n",
    "            # Select a secondary topic that has cross-topic terms with the primary\n",
    "            candidates = [t for t in topic_names if t != primary_topic and (primary_topic, t) in cross_topic_terms or (t, primary_topic) in cross_topic_terms]\n",
    "            if candidates:\n",
    "                secondary_topic = np.random.choice(candidates)\n",
    "                doc_subtopics.append(secondary_topic)\n",
    "                \n",
    "                # Get cross-topic terms\n",
    "                if (primary_topic, secondary_topic) in cross_topic_terms:\n",
    "                    terms = cross_topic_terms[(primary_topic, secondary_topic)]\n",
    "                else:\n",
    "                    terms = cross_topic_terms[(secondary_topic, primary_topic)]\n",
    "                \n",
    "                # Add some cross-topic terms\n",
    "                for term in np.random.choice(terms, size=min(3, len(terms)), replace=False):\n",
    "                    words.append(term)\n",
    "            else:\n",
    "                doc_subtopics.append(\"None\")\n",
    "        else:\n",
    "            doc_subtopics.append(\"None\")\n",
    "        \n",
    "        # Create variations by adding noise and varying length\n",
    "        num_words = len(words) + np.random.randint(-3, 10)\n",
    "        if num_words < 5:\n",
    "            num_words = 5\n",
    "            \n",
    "        # Select random words with replacement and shuffle for more realistic text\n",
    "        selected_words = list(np.random.choice(words, size=num_words, replace=True))\n",
    "        np.random.shuffle(selected_words)\n",
    "        \n",
    "        # Add some random transitional words for more natural text\n",
    "        transitions = [\"and\", \"also\", \"including\", \"with\", \"for\", \"about\", \"regarding\", \n",
    "                      \"related to\", \"concerning\", \"in terms of\", \"specifically\"]\n",
    "        for i in range(2, len(selected_words), 5):\n",
    "            if i < len(selected_words):\n",
    "                selected_words[i] = np.random.choice(transitions)\n",
    "        \n",
    "        document = \" \".join(selected_words)\n",
    "        documents.append(document)\n",
    "        doc_ids.append(f\"doc_{doc_id}\")\n",
    "        doc_id += 1\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        \"text\": documents,\n",
    "        \"id\": doc_ids,\n",
    "        \"primary_topic\": doc_topics,\n",
    "        \"secondary_topic\": doc_subtopics\n",
    "    })\n",
    "    \n",
    "    print(f\"Generated {len(df)} documents across {len(topic_names)} primary topics\")\n",
    "    return df\n",
    "\n",
    "# Generate the sample data\n",
    "df = generate_quality_sample_data(n_samples=300)\n",
    "\n",
    "# Display a few sample documents\n",
    "print(\"\\nSample documents:\")\n",
    "for topic in df[\"primary_topic\"].unique():\n",
    "    sample = df[df[\"primary_topic\"] == topic].sample(1)\n",
    "    secondary = sample[\"secondary_topic\"].values[0]\n",
    "    secondary_info = f\" (with {secondary} influence)\" if secondary != \"None\" else \"\"\n",
    "    print(f\"\\n{topic}{secondary_info}: {sample['text'].values[0]}\")\n",
    "\n",
    "# Show topic distribution\n",
    "print(\"\\nPrimary topic distribution:\")\n",
    "display(df[\"primary_topic\"].value_counts())\n",
    "\n",
    "# Show secondary topic influence\n",
    "print(\"\\nSecondary topic influence:\")\n",
    "display(df[\"secondary_topic\"].value_counts())\n",
    "\n",
    "# Save the data for reference\n",
    "df.to_csv(OUTPUT_DIR / \"quality_sample_data.csv\", index=False)\n",
    "print(f\"\\nSample data saved to {OUTPUT_DIR / 'quality_sample_data.csv'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Initialize the Workflow\n",
    "\n",
    "Now we'll set up the MenoWorkflow with quality-first settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the workflow with quality-first settings\n",
    "print(\"Initializing MenoWorkflow with quality-first settings...\")\n",
    "start_time = time.time()\n",
    "\n",
    "workflow = MenoWorkflow(\n",
    "    config_overrides=QUALITY_CONFIG,\n",
    "    local_model_path=LOCAL_MODEL_PATH,\n",
    "    local_files_only=True,\n",
    "    offline_mode=True\n",
    ")\n",
    "\n",
    "# Load the data\n",
    "workflow.load_data(data=df, text_column=\"text\", id_column=\"id\")\n",
    "print(f\"Loaded {len(df)} documents into the workflow\")\n",
    "\n",
    "# Measure initialization time\n",
    "init_time = time.time() - start_time\n",
    "print(f\"Initialization completed in {init_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Preprocessing\n",
    "\n",
    "Generate preprocessing reports and process the documents with thorough preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start preprocessing timer\n",
    "start_time = time.time()\n",
    "\n",
    "print(\"Preprocessing documents with extensive cleaning...\")\n",
    "workflow.preprocess_documents(\n",
    "    lowercase=True,\n",
    "    remove_punctuation=True,\n",
    "    remove_stopwords=True,\n",
    "    lemmatize=True,\n",
    "    remove_numbers=True\n",
    ")\n",
    "\n",
    "# Get the preprocessed data\n",
    "preprocessed_df = workflow.get_preprocessed_data()\n",
    "print(f\"Preprocessing completed for {len(preprocessed_df)} documents\")\n",
    "\n",
    "# Display sample of preprocessed text\n",
    "print(\"\\nSample of preprocessed text:\")\n",
    "sample_processed = preprocessed_df[[\"text\", \"processed_text\"]].head(3)\n",
    "display(sample_processed)\n",
    "\n",
    "# Measure preprocessing time\n",
    "preproc_time = time.time() - start_time\n",
    "print(f\"Preprocessing completed in {preproc_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. High-Quality Topic Modeling with BERTopic\n",
    "\n",
    "Run high-quality topic modeling using BERTopic with UMAP and HDBSCAN."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Start topic modeling timer\nstart_time = time.time()\n\nprint(\"Discovering topics with high-quality settings (UMAP + HDBSCAN)...\")\nworkflow.discover_topics(\n    method=\"embedding_cluster\",  # Use BERTopic for high-quality results\n    clustering_method=\"hdbscan\",\n    min_topic_size=5,\n    num_topics=\"auto\"  # Let HDBSCAN determine optimal topic count\n)\n\n# Get topic information\ntopics_df = workflow.get_topic_assignments()\ntopic_info = workflow.modeler.get_topic_info()\nprint(f\"\\nDiscovered {len(topic_info) - 1} meaningful topics automatically\")\n\n# Display topic distribution\nprint(\"\\nTopic distribution:\")\ndisplay(topic_info[['Topic', 'Size', 'Name']])\n\n# Display top words per topic\nprint(\"\\nTop words per topic:\")\nfor _, row in topic_info.iterrows():\n    topic_id = row[\"Topic\"]\n    if topic_id != -1:  # Skip outlier topic\n        topic_words = workflow.modeler.get_topic_words(topic_id, top_n=10)\n        word_str = \", \".join([word for word, _ in topic_words[:10]])\n        print(f\"Topic {topic_id} ({row['Size']} docs): {word_str}\")\n\n# Compare with ground truth\nif \"primary_topic\" in df.columns:\n    # Get document assignment with original IDs\n    doc_topics = topics_df.merge(df[[\"id\", \"primary_topic\"]], on=\"id\")\n    \n    # Show contingency table\n    print(\"\\nContingency table (Discovered vs. Actual):\")\n    contingency = pd.crosstab(doc_topics[\"topic\"], doc_topics[\"primary_topic\"])\n    display(contingency)\n    \n    # Calculate adjusted mutual information\n    from sklearn.metrics import adjusted_mutual_info_score\n    ami = adjusted_mutual_info_score(\n        doc_topics[\"topic\"].apply(lambda x: str(x)), \n        doc_topics[\"primary_topic\"]\n    )\n    print(f\"\\nAdjusted Mutual Information: {ami:.4f}\")\n\n# Measure topic modeling time\ntopic_time = time.time() - start_time\nprint(f\"\\nTopic modeling completed in {topic_time:.2f} seconds\")",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Enhanced Topic Labeling\n",
    "\n",
    "Create more descriptive topic labels based on the top keywords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced topic labeling function for better descriptions\n",
    "def generate_enhanced_topic_label(topic_words, topic_id):\n",
    "    \"\"\"Generate a more descriptive topic label from keywords.\"\"\"\n",
    "    if topic_id == -1:\n",
    "        return \"Miscellaneous/Outliers\"\n",
    "    \n",
    "    # Get key terms with weights\n",
    "    keywords = [word for word, _ in topic_words[:5]]\n",
    "    weights = [weight for _, weight in topic_words[:5]]\n",
    "    \n",
    "    # Check for specific domain indicators\n",
    "    domains = {\n",
    "        \"ai\": [\"ai\", \"artificial\", \"intelligence\", \"machine\", \"learning\", \"neural\", \"deep\"],\n",
    "        \"healthcare\": [\"medical\", \"health\", \"patient\", \"clinical\", \"hospital\", \"doctor\"],\n",
    "        \"finance\": [\"financial\", \"finance\", \"banking\", \"investment\", \"market\", \"trading\"],\n",
    "        \"energy\": [\"energy\", \"power\", \"renewable\", \"sustainable\", \"carbon\", \"grid\"],\n",
    "        \"data\": [\"data\", \"analysis\", \"analytics\", \"processing\", \"visualization\"]\n",
    "    }\n",
    "    \n",
    "    # Check if keywords match any domains\n",
    "    domain_matches = {}\n",
    "    for domain, terms in domains.items():\n",
    "        matches = sum(1 for kw in keywords if kw in terms)\n",
    "        if matches > 0:\n",
    "            domain_matches[domain] = matches\n",
    "    \n",
    "    # If we have domain matches, use the best one as prefix\n",
    "    if domain_matches:\n",
    "        best_domain = max(domain_matches.items(), key=lambda x: x[1])[0]\n",
    "        domain_prefix = {\n",
    "            \"ai\": \"AI & \",\n",
    "            \"healthcare\": \"Healthcare & \",\n",
    "            \"finance\": \"Financial & \",\n",
    "            \"energy\": \"Energy & \",\n",
    "            \"data\": \"Data Science & \"\n",
    "        }[best_domain]\n",
    "    else:\n",
    "        domain_prefix = \"\"\n",
    "    \n",
    "    # Combine top keywords with weights for emphasis\n",
    "    primary_keywords = [keywords[0], keywords[1]] if len(keywords) > 1 else [keywords[0]]\n",
    "    secondary_keywords = keywords[2:4] if len(keywords) > 3 else keywords[2:]\n",
    "    \n",
    "    # Format the label based on available keywords\n",
    "    if secondary_keywords:\n",
    "        label = f\"{domain_prefix}{' & '.join(primary_keywords).title()} ({', '.join(secondary_keywords)})\"\n",
    "    else:\n",
    "        label = f\"{domain_prefix}{' & '.join(primary_keywords).title()}\"\n",
    "    \n",
    "    return label\n",
    "\n",
    "# Generate enhanced topic labels\n",
    "print(\"Generating enhanced topic labels...\")\n",
    "topic_labels = {}\n",
    "\n",
    "for topic_id in topic_info[\"Topic\"].unique():\n",
    "    # Get top words for this topic\n",
    "    top_words = workflow.modeler.get_topic_words(topic_id, top_n=10) if topic_id != -1 else []\n",
    "    \n",
    "    # Generate enhanced label\n",
    "    label = generate_enhanced_topic_label(top_words, topic_id)\n",
    "    topic_labels[topic_id] = label\n",
    "\n",
    "# Print enhanced labels\n",
    "print(\"\\nEnhanced topic labels:\")\n",
    "for topic_id, label in topic_labels.items():\n",
    "    if topic_id != -1:  # Skip outlier topic in display\n",
    "        topic_size = topic_info[topic_info[\"Topic\"] == topic_id][\"Size\"].values[0]\n",
    "        print(f\"Topic {topic_id} ({topic_size} docs): {label}\")\n",
    "\n",
    "# Save topic information with enhanced labels\n",
    "topic_info[\"Enhanced_Label\"] = topic_info[\"Topic\"].map(topic_labels)\n",
    "topic_info.to_csv(OUTPUT_DIR / \"topic_summary_enhanced.csv\", index=False)\n",
    "print(f\"\\nEnhanced topic summary saved to {OUTPUT_DIR / 'topic_summary_enhanced.csv'}\")\n",
    "\n",
    "# Save document-topic assignments with enhanced labels\n",
    "topics_df[\"Topic_Label\"] = topics_df[\"topic\"].map(topic_labels)\n",
    "topics_df.to_csv(OUTPUT_DIR / \"document_topics_enhanced.csv\", index=False)\n",
    "print(f\"Document-topic assignments saved to {OUTPUT_DIR / 'document_topics_enhanced.csv'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Generate Comprehensive HTML Report\n",
    "\n",
    "Create a high-quality HTML report with all the topic modeling results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start report generation timer\n",
    "start_time = time.time()\n",
    "\n",
    "print(\"Generating comprehensive report with enhanced visualizations...\")\n",
    "report_path = workflow.generate_comprehensive_report(\n",
    "    output_path=OUTPUT_DIR / \"high_quality_topic_report.html\",\n",
    "    open_browser=False,\n",
    "    title=\"High-Quality CPU Topic Analysis Report\",\n",
    "    include_interactive=True,\n",
    "    topic_labels=topic_labels  # Use our enhanced labels\n",
    ")\n",
    "\n",
    "print(f\"Report generated at {report_path}\")\n",
    "\n",
    "# Measure report generation time\n",
    "report_time = time.time() - start_time\n",
    "print(f\"Report generation completed in {report_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. High-Quality BERTopic Visualizations\n",
    "\n",
    "Create detailed UMAP-based visualizations that prioritize quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start visualization timer\n",
    "start_time = time.time()\n",
    "\n",
    "print(\"Creating high-quality UMAP visualizations (this may take some time)...\")\n",
    "\n",
    "# Get the BERTopic model\n",
    "model = workflow.modeler.topic_model\n",
    "\n",
    "# 3D embedding visualization with UMAP (higher quality than PCA)\n",
    "try:\n",
    "    print(\"\\nGenerating 3D UMAP visualization...\")\n",
    "    embed_fig = workflow.modeler.visualize_embeddings(\n",
    "        return_figure=True,\n",
    "        plot_3d=True,  # 3D visualization for better separation\n",
    "        width=1000,\n",
    "        height=800,\n",
    "        include_topic_labels=True,  # Add topic labels\n",
    "        hover_data=[\"topic\", \"Topic_Label\"],  # Add custom hover info\n",
    "        topic_label_dict=topic_labels  # Use enhanced labels\n",
    "    )\n",
    "    embed_fig.write_html(OUTPUT_DIR / \"3d_topic_embeddings.html\")\n",
    "    print(f\"3D UMAP visualization saved to {OUTPUT_DIR / '3d_topic_embeddings.html'}\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not create 3D UMAP visualization: {e}\")\n",
    "\n",
    "# Topic similarity network (high-quality visualization)\n",
    "try:\n",
    "    print(\"\\nGenerating topic similarity network...\")\n",
    "    network_fig = model.visualize_topics(\n",
    "        topics=\"all\",  # Include all topics\n",
    "        top_n_topics=None\n",
    "    )\n",
    "    network_fig.write_html(OUTPUT_DIR / \"topic_similarity_network.html\")\n",
    "    print(f\"Topic similarity network saved to {OUTPUT_DIR / 'topic_similarity_network.html'}\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not create topic similarity network: {e}\")\n",
    "\n",
    "# Topic hierarchy visualization (detailed hierarchical clustering)\n",
    "try:\n",
    "    print(\"\\nGenerating topic hierarchy visualization...\")\n",
    "    hierarchy_fig = create_bertopic_hierarchy(\n",
    "        model=model,\n",
    "        orientation=\"left\",\n",
    "        width=1200,\n",
    "        height=800\n",
    "    )\n",
    "    if hierarchy_fig:\n",
    "        hierarchy_fig.write_html(OUTPUT_DIR / \"topic_hierarchy.html\")\n",
    "        print(f\"Topic hierarchy saved to {OUTPUT_DIR / 'topic_hierarchy.html'}\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not create topic hierarchy: {e}\")\n",
    "\n",
    "# Inter-topic distance map (uses UMAP for topic coordinates)\n",
    "try:\n",
    "    print(\"\\nGenerating inter-topic distance map...\")\n",
    "    distance_fig = model.visualize_topics_over_time(\n",
    "        topics=\"all\",\n",
    "        top_n_topics=None,\n",
    "        custom_labels=topic_labels,\n",
    "        width=1000,\n",
    "        height=800\n",
    "    )\n",
    "    distance_fig.write_html(OUTPUT_DIR / \"topic_distance_map.html\")\n",
    "    print(f\"Inter-topic distance map saved to {OUTPUT_DIR / 'topic_distance_map.html'}\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not create inter-topic distance map: {e}\")\n",
    "\n",
    "# Term score decline curves\n",
    "try:\n",
    "    print(\"\\nGenerating term score decline curves...\")\n",
    "    for topic_id in [t for t in topic_info[\"Topic\"].unique() if t != -1][:5]:  # Show first 5 topics\n",
    "        term_fig = model.visualize_term_rank(topic=topic_id)\n",
    "        term_fig.write_html(OUTPUT_DIR / f\"term_rank_topic_{topic_id}.html\")\n",
    "        print(f\"Term rank curve for Topic {topic_id} saved\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not create term score decline curves: {e}\")\n",
    "\n",
    "# Measure visualization time\n",
    "viz_time = time.time() - start_time\n",
    "print(f\"\\nHigh-quality visualizations completed in {viz_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Advanced Topic Analysis\n",
    "\n",
    "Perform deeper analysis of the topic structure and document assignments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start advanced analysis timer\n",
    "start_time = time.time()\n",
    "\n",
    "print(\"Performing advanced topic analysis...\")\n",
    "\n",
    "# Extract and save representative documents per topic\n",
    "representative_docs = []\n",
    "for topic_id in [t for t in topic_info[\"Topic\"].unique() if t != -1]:  # Skip outlier topic\n",
    "    # Get top documents for this topic\n",
    "    try:\n",
    "        topic_docs = model.get_representative_docs(topic=topic_id, nr_docs=3)\n",
    "        \n",
    "        for doc in topic_docs:\n",
    "            representative_docs.append({\n",
    "                \"topic_id\": topic_id,\n",
    "                \"topic_label\": topic_labels.get(topic_id, f\"Topic {topic_id}\"),\n",
    "                \"document\": doc\n",
    "            })\n",
    "    except Exception as e:\n",
    "        print(f\"Could not get representative docs for Topic {topic_id}: {e}\")\n",
    "        continue\n",
    "\n",
    "# Create DataFrame of representative documents\n",
    "if representative_docs:\n",
    "    rep_docs_df = pd.DataFrame(representative_docs)\n",
    "    rep_docs_df.to_csv(OUTPUT_DIR / \"representative_documents.csv\", index=False)\n",
    "    print(f\"\\nSaved {len(representative_docs)} representative documents to {OUTPUT_DIR / 'representative_documents.csv'}\")\n",
    "    \n",
    "    # Display a sample\n",
    "    print(\"\\nSample representative documents:\")\n",
    "    display(rep_docs_df.groupby(\"topic_label\").head(1)[[\"topic_label\", \"document\"]])\n",
    "\n",
    "# Calculate topic similarity matrix\n",
    "try:\n",
    "    print(\"\\nCalculating topic similarity matrix...\")\n",
    "    topic_ids = [t for t in topic_info[\"Topic\"].unique() if t != -1]\n",
    "    similarity_matrix = np.zeros((len(topic_ids), len(topic_ids)))\n",
    "    \n",
    "    for i, topic1 in enumerate(topic_ids):\n",
    "        for j, topic2 in enumerate(topic_ids):\n",
    "            if i == j:\n",
    "                similarity_matrix[i, j] = 1.0\n",
    "            else:\n",
    "                # Get topic vectors\n",
    "                vector1 = model.topic_vectors.get(topic1, None)\n",
    "                vector2 = model.topic_vectors.get(topic2, None)\n",
    "                \n",
    "                if vector1 is not None and vector2 is not None:\n",
    "                    # Calculate cosine similarity\n",
    "                    similarity = np.dot(vector1, vector2) / (np.linalg.norm(vector1) * np.linalg.norm(vector2))\n",
    "                    similarity_matrix[i, j] = similarity\n",
    "    \n",
    "    # Create similarity DataFrame with enhanced labels\n",
    "    topic_labels_list = [topic_labels.get(t, f\"Topic {t}\") for t in topic_ids]\n",
    "    similarity_df = pd.DataFrame(similarity_matrix, index=topic_labels_list, columns=topic_labels_list)\n",
    "    \n",
    "    # Save to CSV\n",
    "    similarity_df.to_csv(OUTPUT_DIR / \"topic_similarity_matrix.csv\")\n",
    "    print(f\"Topic similarity matrix saved to {OUTPUT_DIR / 'topic_similarity_matrix.csv'}\")\n",
    "    \n",
    "    # Create heatmap visualization of topic similarity\n",
    "    import plotly.graph_objects as go\n",
    "    fig = go.Figure(data=go.Heatmap(\n",
    "        z=similarity_matrix,\n",
    "        x=topic_labels_list,\n",
    "        y=topic_labels_list,\n",
    "        colorscale='Viridis',\n",
    "        showscale=True\n",
    "    ))\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=\"Topic Similarity Matrix\",\n",
    "        width=1000,\n",
    "        height=800,\n",
    "        xaxis=dict(title=\"Topic\"),\n",
    "        yaxis=dict(title=\"Topic\")\n",
    "    )\n",
    "    \n",
    "    fig.write_html(OUTPUT_DIR / \"topic_similarity_heatmap.html\")\n",
    "    print(f\"Topic similarity heatmap saved to {OUTPUT_DIR / 'topic_similarity_heatmap.html'}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Could not calculate topic similarity matrix: {e}\")\n",
    "\n",
    "# Measure advanced analysis time\n",
    "analysis_time = time.time() - start_time\n",
    "print(f\"\\nAdvanced analysis completed in {analysis_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Performance Summary\n",
    "\n",
    "Summarize the performance metrics of our quality-focused workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create performance summary\n",
    "print(\"Performance Summary\")\n",
    "print(\"===================\\n\")\n",
    "print(f\"Dataset size: {len(df)} documents\")\n",
    "print(f\"Topics discovered: {len(topic_info) - 1} (excluding outliers)\")\n",
    "print(\"\\nProcessing times:\")\n",
    "print(f\"- Initialization: {init_time:.2f} seconds\")\n",
    "print(f\"- Preprocessing: {preproc_time:.2f} seconds\")\n",
    "print(f\"- Topic modeling: {topic_time:.2f} seconds\")\n",
    "print(f\"- Report generation: {report_time:.2f} seconds\")\n",
    "print(f\"- Visualizations: {viz_time:.2f} seconds\")\n",
    "print(f\"- Advanced analysis: {analysis_time:.2f} seconds\")\n",
    "print(f\"- Total processing time: {init_time + preproc_time + topic_time + report_time + viz_time + analysis_time:.2f} seconds\")\n",
    "\n",
    "if \"primary_topic\" in df.columns:\n",
    "    print(f\"\\nAdjusted Mutual Information: {ami:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Summary and Conclusion\n",
    "\n",
    "This notebook demonstrated a quality-first approach to CPU-bound topic modeling using Meno. We prioritized result quality over processing speed by using:\n",
    "\n",
    "1. **Full-featured embedding models**: Using `all-MiniLM-L6-v2` without quantization for best embedding quality\n",
    "2. **UMAP dimensionality reduction**: Slower but produces superior topic separation vs. PCA\n",
    "3. **High-quality BERTopic**: Using HDBSCAN clustering for optimal topic coherence\n",
    "4. **Enhanced visualizations**: Creating detailed, information-rich visualizations\n",
    "5. **Advanced topic analysis**: Performing deeper analysis of topic structure and relationships\n",
    "\n",
    "### Key Benefits of this Approach\n",
    "\n",
    "- **Superior topic separation**: Better distinguishes between related topics\n",
    "- **Higher topic coherence**: Topics are more internally consistent\n",
    "- **More detailed visualizations**: Richer visual representations of topic relationships\n",
    "- **Enhanced topic labels**: More meaningful descriptions of topic content\n",
    "- **Works entirely on CPU**: No GPU required, just more processing time\n",
    "\n",
    "This approach is ideal when you prioritize result quality over processing speed, especially for more nuanced datasets where topics have subtle differences or overlap."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}