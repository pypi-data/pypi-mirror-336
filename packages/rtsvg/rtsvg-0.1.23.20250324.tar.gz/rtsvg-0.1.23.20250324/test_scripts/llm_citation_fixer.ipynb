{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLM(object):\n",
    "    def promptModel(self, prompt, sytem_prompt=\"You are a helpful AI assistant.\", max_tokens=512, temperature=0.0):\n",
    "        raise Exception('LLM Object Needs Subclassed')\n",
    "\n",
    "#\n",
    "# MISTRAL NEMO Instruct\n",
    "#\n",
    "class LLMNemoInstruct(LLM):\n",
    "    def __init__(self):\n",
    "        from huggingface_hub import snapshot_download\n",
    "        from pathlib import Path\n",
    "        mistral_models_path = Path.home().joinpath('mistral_models', 'Nemo-Instruct')\n",
    "        mistral_models_path.mkdir(parents=True, exist_ok=True)\n",
    "        snapshot_download(repo_id=\"mistralai/Mistral-Nemo-Instruct-2407\", allow_patterns=[\"params.json\", \"consolidated.safetensors\", \"tekken.json\"], local_dir=mistral_models_path)\n",
    "        from mistral_inference.transformer import Transformer\n",
    "        from mistral_common.tokens.tokenizers.mistral import MistralTokenizer\n",
    "        self.tokenizer = MistralTokenizer.from_file(f\"{mistral_models_path}/tekken.json\")\n",
    "        self.model     = Transformer.from_folder(mistral_models_path)\n",
    "    def promptModel(self, prompt, system_prompt=\"You are a helpful AI assistant.\", max_tokens=512, temperature=0.0):\n",
    "        from mistral_common.protocol.instruct.request import ChatCompletionRequest\n",
    "        from mistral_common.protocol.instruct.messages import UserMessage\n",
    "        from mistral_inference.generate import generate\n",
    "        completion_request = ChatCompletionRequest(messages=[UserMessage(content=prompt)])\n",
    "        tokens = self.tokenizer.encode_chat_completion(completion_request).tokens\n",
    "        out_tokens, _ = generate([tokens], self.model, max_tokens=max_tokens, temperature=temperature, eos_id=self.tokenizer.instruct_tokenizer.tokenizer.eos_id)\n",
    "        result = self.tokenizer.instruct_tokenizer.tokenizer.decode(out_tokens[0])\n",
    "        return result\n",
    "\n",
    "#\n",
    "# LLAMA 3.1 8B Instruct\n",
    "#\n",
    "class LLMLlama31_8b(LLM):\n",
    "    def __init__(self):\n",
    "        import transformers\n",
    "        import torch\n",
    "        model_id = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "        self.pipeline = transformers.pipeline(\n",
    "            \"text-generation\",\n",
    "            model=model_id,\n",
    "            model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "            device_map=\"auto\",\n",
    "        )\n",
    "    def promptModel(self, prompt, system_prompt=\"You are a helpful AI assistant.\", max_tokens=512, temperature=0.0):\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\",   \"content\": prompt},\n",
    "        ]\n",
    "        outputs = self.pipeline(\n",
    "            messages,\n",
    "            max_new_tokens=max_tokens,\n",
    "            do_sample=False,\n",
    "            temperature=temperature\n",
    "        )\n",
    "        return outputs[0][\"generated_text\"][-1]['content']\n",
    "\n",
    "#\n",
    "# Microsoft's Phi 3 Small 8K Instruct\n",
    "# - fails due to a cudnn library issue\n",
    "#\n",
    "class LLMPhi3Small8k(LLM):\n",
    "    def __init__(self):\n",
    "        import torch\n",
    "        from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "        torch.random.manual_seed(0)\n",
    "        model_id = \"microsoft/Phi-3-small-8k-instruct\"\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_id, \n",
    "            torch_dtype=\"auto\", \n",
    "            trust_remote_code=True, \n",
    "        )\n",
    "        assert torch.cuda.is_available(), \"This model needs a GPU to run ...\"\n",
    "        self.device    = torch.cuda.current_device()\n",
    "        self.model     = self.model.to(self.device)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    def promptModel(self, prompt, system_prompt=\"You are a helpful AI assistant.\", max_tokens=512, temperature=0.0):\n",
    "        from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "        messages = [\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "        pipe = pipeline(\n",
    "            \"text-generation\",\n",
    "            model=self.model,\n",
    "            tokenizer=self.tokenizer,\n",
    "            device=self.device\n",
    "        )\n",
    "        generation_args = {\n",
    "            \"max_new_tokens\": max_tokens,\n",
    "            \"return_full_text\": False,\n",
    "            \"temperature\": temperature,\n",
    "            \"do_sample\": False,\n",
    "        }\n",
    "\n",
    "        output = pipe(messages, **generation_args)\n",
    "        return output[0]['generated_text']\n",
    "\n",
    "#llm = LLMNemoInstruct()\n",
    "llm = LLMLlama31_8b()\n",
    "#llm = LLMPhi3Small8k() # fails due to cudnn library issue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prompt = \"How expensive would it be to ask a window cleaner to clean all windows in Paris. Make a reasonable guess in US Dollar.\"\n",
    "#completion_request = ChatCompletionRequest(messages=[UserMessage(content=prompt)])\n",
    "#tokens = tokenizer.encode_chat_completion(completion_request).tokens\n",
    "#out_tokens, _ = generate([tokens], model, max_tokens=64, temperature=0.35, eos_id=tokenizer.instruct_tokenizer.tokenizer.eos_id)\n",
    "#result = tokenizer.decode(out_tokens[0])\n",
    "#print(result)\n",
    "llm.promptModel('How many \"r\"s are there in strawberry?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts, original_citations, shortened_citations = [], [], []\n",
    "#\n",
    "# Source For Original Citations: https://en.wikipedia.org/wiki/Apollo_13\n",
    "#\n",
    "texts               .append('Apollo 13 was led by J. Lovell.')\n",
    "original_citations  .append('The mission was commanded by Jim Lovell, with Jack Swigert as command module (CM) pilot and Fred Haise as lunar module (LM) pilot. Swigert was a late replacement for Ken Mattingly, who was grounded after exposure to rubella. ')\n",
    "shortened_citations .append('The mission was commanded by Jim Lovell') # ideal answer\n",
    "\n",
    "texts               .append('Apollo looped around the Moon instead.')\n",
    "original_citations  .append('The crew, supported by backup systems on the lunar module (LM), instead looped around the Moon in a circumlunar trajectory and returned safely to Earth on April 17.')\n",
    "shortened_citations .append('instead looped around the Moon') # ideal answer\n",
    "\n",
    "texts               .append('They returned safely to Earth.')\n",
    "original_citations  .append('The crew, supported by backup systems on the lunar module (LM), instead looped around the Moon in a circumlunar trajectory and returned safely to Earth on April 17.')\n",
    "shortened_citations .append('returned safely to Earth') # ideal answer\n",
    "\n",
    "#\n",
    "# Source for Original Citations:  https://en.wikipedia.org/wiki/War_and_Peace\n",
    "#\n",
    "texts               .append('Authors did not always agree upon what comprised a novel -- for example, Tolstoy indicated that War and Peace was not a novel.')\n",
    "original_citations  .append('Tolstoy said that the best Russian literature does not conform to standards and hence hesitated to classify War and Peace, saying it is \"not a novel, even less is it a poem, and still less a historical chronicle.\"')\n",
    "shortened_citations .append('Tolstoy said that the best Russian literature does not conform to standards and hence hesitated to classify War and Peace, saying it is \"not a novel') # ideal answer\n",
    "\n",
    "#texts               .append('')\n",
    "#original_citations  .append('')\n",
    "#shortened_citations .append('')\n",
    "\n",
    "def cleanResponse(s):\n",
    "    _prefixes_ = ['The shortened citation would be:',\n",
    "                  'Shortened Citation:',]\n",
    "    for _prefix_ in _prefixes_:\n",
    "        if s.startswith(_prefix_):\n",
    "            s = s[len(_prefix_):].strip()\n",
    "    if s.startswith(\"'\") and s.endswith(\"'\"): return s[1:-1]\n",
    "    if s.startswith('\"') and s.endswith('\"'): return s[1:-1]\n",
    "    return s\n",
    "\n",
    "def formatPrompt(text, original_citation):\n",
    "    return 'not defined yet'\n",
    "\n",
    "def runPrompts():\n",
    "    _responses_, max_response_lenght = [], 0\n",
    "    for i in range(len(texts)):\n",
    "        _text_              = texts[i]\n",
    "        _original_citation_ = original_citations[i]\n",
    "        _model_response_    = llm.promptModel(formatPrompt(_text_, _original_citation_))\n",
    "        _model_response_    = cleanResponse(_model_response_)\n",
    "        _responses_.append(_model_response_)\n",
    "        max_response_lenght = max(max_response_lenght, len(_model_response_))\n",
    "    for i in range(len(texts)):\n",
    "        _model_response_    = _responses_[i]\n",
    "        _original_citation_ = original_citations[i]\n",
    "        _spaces_            = (max_response_lenght - len(_model_response_)) * ' '\n",
    "        print(f'\"{_model_response_}\" {_spaces_} {_model_response_ in _original_citation_}  \"{shortened_citations[i]}\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatPrompt(text, original_citation): \n",
    "    return  \"My citations are too long.  \" + \\\n",
    "            \"Shorten the following citation to just the part that supports the phrase.  \" + \\\n",
    "           f\"The citation should use the exact words in the supplied citation.\\n\\nPhrase: '{text}'\\n\\nSupplied Citation: '{original_citation}'\"\n",
    "runPrompts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ChatGPT Recommended Prompt \"make me a prompt for a language model that takes a citation that is too long and shortens it to only the necessary part for a given text.\"\n",
    "def formatPrompt(text, original_citation): return f'''Given a citation that is too lengthy, shorten it to include only the essential information needed to support the main point of the following text. Ensure the shortened citation retains the key details, remains accurate, and provides sufficient context.\n",
    "\n",
    "Text:\n",
    "\n",
    "{text}\n",
    "\n",
    "Original Citation:\n",
    "\n",
    "{original_citation}\n",
    "\n",
    "Shortened Citation:'''\n",
    "runPrompts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Claude Recommendation (heavily modified to remove formal citatino language)\n",
    "def formatPrompt(text, original_citation): return f\"\"\"You are an AI assistant specialized in academic writing and citation management. Your task is to analyze a given piece of text and a lengthy citation, then shorten the citation to include only the parts that are directly relevant to the text.\n",
    "\n",
    "Text:\n",
    "'{text}'\n",
    "\n",
    "Lengthy Citation:\n",
    "'{original_citation}'\n",
    "\n",
    "Shortened Citation:\"\"\"\n",
    "runPrompts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatPrompt(text, original_citation):\n",
    "    return f'Return only the substring from \"Lengthy Text\" that supports the following statement: \"{text}\"\\n\\nLengthy text: \"{original_citation}\"'\n",
    "runPrompts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatPrompt(text, original_citation):\n",
    "    return f'For the following statement:\\n\\n\"{text}\"\\n\\nWhat is the shortest substring from the following that supports that statement:\\n\\n\"{original_citation}\"\\n\\nOnly provide the substring, no reasoning or caveats.'\n",
    "runPrompts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
