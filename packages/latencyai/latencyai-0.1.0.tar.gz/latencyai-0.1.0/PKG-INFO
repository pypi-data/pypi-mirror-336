Metadata-Version: 2.3
Name: latencyai
Version: 0.1.0
Summary: AI Performance Engineer
License: MIT
Keywords: Performance engineering,Performance tuning
Author: Graphsignal, Inc.
Author-email: devops@graphsignal.com
Requires-Python: >=3.9
Classifier: Development Status :: 5 - Production/Stable
Classifier: Intended Audience :: Developers
Classifier: Intended Audience :: Science/Research
Classifier: License :: OSI Approved :: Apache Software License
Classifier: License :: OSI Approved :: MIT License
Classifier: Operating System :: OS Independent
Classifier: Programming Language :: Python
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.9
Classifier: Programming Language :: Python :: 3.10
Classifier: Programming Language :: Python :: 3.11
Classifier: Programming Language :: Python :: 3.12
Classifier: Programming Language :: Python :: 3.13
Classifier: Topic :: Scientific/Engineering
Classifier: Topic :: Scientific/Engineering :: Artificial Intelligence
Classifier: Topic :: Software Development
Classifier: Topic :: System :: Monitoring
Requires-Dist: openai
Requires-Dist: pydantic (>2)
Requires-Dist: python-dateutil
Requires-Dist: torch
Requires-Dist: typing-extensions (>4.7.1)
Project-URL: Homepage, https://graphsignal.com
Project-URL: Repository, https://graphsignal.com
Description-Content-Type: text/markdown

# LatencyAI - AI Performance Engineer

## Introduction

LatencyAI is an AI agent that optimizes any Python code for best performance using reasoning LLMs. It iteratively profiles, optimizes, and benchmarks the code. The goal is to optimize code by GPU offloading, using data/task parallel, latency hiding and other techniques.

**Note: this is an experimental library.**

## Installation

* (Optional) Deploy a CUDA-enabled GPU instance
* `pip install --upgrade latencyai`

## Usage

* Set the OPEANAI_API_KEY environment variable
* Run `python -m latencyai --runs=3 script-to-optimize.py`. Optionally set `--runs`, which is the number of optimization attempts, i.e. optimize-benchmark-profile iterations. The default is 2.

The provided script should have a `main` function. The benchmark runner calls it multiple times, depending on it's execution time.

If optimization is successful, a file named `<original-script>_optimized.py` is be written to original script directory.

## Tracking optimizations

After integrating optimized code into your application, you can verify and track end-to-end performance improvements in deployed applications using [Graphsignal](https://graphsignal.com/).
