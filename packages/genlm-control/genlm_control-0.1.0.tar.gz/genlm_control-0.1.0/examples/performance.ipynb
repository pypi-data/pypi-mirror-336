{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance optimizations\n",
    "\n",
    "This notebook describes two available performance optimizations for instances of the `Potential` class:\n",
    "\n",
    "* **Autobatching**: Batches concurrent requests to the `complete`, `prefix` and `logw_next` methods.\n",
    "* **Multiprocessing**: Runs multiple instances of a `Potential` in parallel across different CPU cores.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import asyncio\n",
    "import numpy as np\n",
    "from arsenal.timer import timeit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autobatching concurrent requests\n",
    "\n",
    "Autobatching can be used to improve the performance of a `Potential` class in which the batch methods (`batch_complete` and `batch_prefix`) are much more efficient than sequentially running the individual instance methods. \n",
    "\n",
    "Consider the following `Potential` class, in which the `complete` and `prefix` methods each sleep for 0.5 seconds, but the batch methods each sleep for 0.55 seconds.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-25 15:21:36 __init__.py:183] Automatically detected platform cuda.\n"
     ]
    }
   ],
   "source": [
    "from genlm_control.potential import Potential\n",
    "\n",
    "\n",
    "class TimedPotential(Potential):\n",
    "    async def complete(self, context):\n",
    "        time.sleep(0.5)\n",
    "        return len(context)\n",
    "\n",
    "    async def prefix(self, context):\n",
    "        time.sleep(0.5)\n",
    "        return len(context)\n",
    "\n",
    "    # Batched methods are much quicker than sequentially\n",
    "    # calling the instance methods.\n",
    "\n",
    "    async def batch_complete(self, contexts):\n",
    "        time.sleep(0.55)\n",
    "        return [len(context) for context in contexts]\n",
    "\n",
    "    async def batch_prefix(self, contexts):\n",
    "        time.sleep(0.55)\n",
    "        return [len(context) for context in contexts]\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"TimedPotential()\"\n",
    "\n",
    "\n",
    "potential = TimedPotential(list(range(256)))  # Vocabulary of bytes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `to_autobatched()` method creates a wrapper around any `Potential` that automatically batches concurrent requests. When multiple requests are made concurrently (like using `asyncio.gather`), the wrapper collects these requests in the background and processes them together using the potential's batch methods. This happens transparently - you don't need to change how you write your code, just wrap your potential with `to_autobatched()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AutoBatchedPotential(TimedPotential())"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autobatched = potential.to_autobatched()\n",
    "autobatched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = [b\"hello\", b\"cats\", b\"foo\", b\"fy\"]\n",
    "\n",
    "# Concurrent requests to complete will be automatically batched\n",
    "# and processed by the batch_complete method.\n",
    "\n",
    "with timeit(\"without autobatching\"):\n",
    "    results = await asyncio.gather(*(potential.complete(seq) for seq in sequences))\n",
    "\n",
    "with timeit(\"with autobatching\"):\n",
    "    results_autobatched = await asyncio.gather(\n",
    "        *(autobatched.complete(seq) for seq in sequences)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([5, 4, 3, 2], [5, 4, 3, 2])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Results are the same whether we use autobatching or not.\n",
    "results, results_autobatched"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CPU Parallelization\n",
    "\n",
    "CPU parallelization can be used to improve the performance of a `Potential` class whose methods are compute-intensive. For example, if your potential performs heavy computation for each request to `complete`, `prefix` or `logw_next`, running it across multiple cores can significantly reduce processing time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimedPotential(Potential):\n",
    "    async def complete(self, context):\n",
    "        time.sleep(1)\n",
    "        return len(context)\n",
    "\n",
    "    async def prefix(self, context):\n",
    "        time.sleep(1)\n",
    "        return len(context)\n",
    "\n",
    "    # These are the default implementations of batch_complete and batch_prefix\n",
    "    # which subclasses inherit. We repeat them here for clarity.\n",
    "    async def batch_complete(self, contexts):\n",
    "        results = await asyncio.gather(\n",
    "            *(self.complete(context) for context in contexts)\n",
    "        )\n",
    "        return np.array(results)\n",
    "\n",
    "    async def batch_prefix(self, contexts):\n",
    "        results = await asyncio.gather(*(self.prefix(context) for context in contexts))\n",
    "        return np.array(results)\n",
    "\n",
    "    def spawn(self):\n",
    "        return TimedPotential(self.decode)\n",
    "\n",
    "\n",
    "potential = TimedPotential(list(range(256)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `to_multiprocess()` method creates a wrapper that runs multiple instances of a potential in parallel across different CPU cores. When you call this method with a specified number of workers, it creates a process pool where each worker contains its own instance of the potential. Requests are then automatically distributed across these workers, allowing for parallel processing.\n",
    "\n",
    "Note that for multiprocessing to work, the potential must implement a picklable `spawn` method, which creates a new instance of the potential. This is true for only some of the built-in `Potential` classes, and if you are using a custom potential you will need to implement this method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiProcPotential(self.num_workers=2)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mp_potential = potential.to_multiprocess(num_workers=2)\n",
    "mp_potential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiprocessing improves performance for both the batched (`batch_complete`, `batch_prefix`, `batch_logw_next`) and unbatched (`complete`, `prefix`, `logw_next`) methods. In the batched case, each request in the batch is processed in parallel across different workers. For individual method calls (like `complete` or `prefix`), each request is sent to an available worker process and executed independently, allowing multiple requests to run in parallel without blocking each other.\n",
    "\n",
    "Here we compare the performance of the batched methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "without multiprocessing (4.0025 sec)\n",
      "with multiprocessing (2.0028 sec)\n"
     ]
    }
   ],
   "source": [
    "with timeit(\"without multiprocessing\"):\n",
    "    results = await potential.batch_complete(sequences)\n",
    "\n",
    "with timeit(\"with multiprocessing\"):\n",
    "    results_mp = await mp_potential.batch_complete(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([5, 4, 3, 2], array([5, 4, 3, 2]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results, results_mp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here we compare the performance of the unbatched methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "without multiprocessing (4.0008 sec)\n",
      "with multiprocessing (2.0021 sec)\n"
     ]
    }
   ],
   "source": [
    "with timeit(\"without multiprocessing\"):\n",
    "    results = await asyncio.gather(*(potential.complete(seq) for seq in sequences))\n",
    "\n",
    "with timeit(\"with multiprocessing\"):\n",
    "    results_mp = await asyncio.gather(\n",
    "        *(mp_potential.complete(seq) for seq in sequences)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([5, 4, 3, 2], [5, 4, 3, 2])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results, results_mp"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
