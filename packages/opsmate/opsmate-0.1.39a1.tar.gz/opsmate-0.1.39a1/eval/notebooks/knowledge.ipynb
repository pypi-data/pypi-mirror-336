{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Knowledge eval using the kubernetes website data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from opsmate.libs.knowledge import (\n",
    "    Runbook,\n",
    "    get_runbooks_table,\n",
    "    DatabaseConnection,\n",
    "    DocumentIngester,\n",
    ")\n",
    "from opsmate.libs.core.types import DocumentIngestion, DocumentIngestionSpec, Metadata\n",
    "from opsmate.libs.config import config\n",
    "import pandas as pd\n",
    "from pydantic import BaseModel\n",
    "config.embeddings_db_path = \"./opsmate-embedding\"\n",
    "config.embedding_registry_name = \"openai\"\n",
    "config.embedding_model_name = \"text-embedding-3-small\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "if [ ! -d \"website\" ]; then\n",
    "    git clone git@github.com:kubernetes/website.git --depth=1\n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's import the kubernetes website data into the embedding database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m2024-11-15 21:40:15\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mbatch ingest runbooks         \u001b[0m \u001b[36mbatch_size\u001b[0m=\u001b[35m100\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m2024-11-15 21:40:17\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mbatch ingest runbooks         \u001b[0m \u001b[36mbatch_size\u001b[0m=\u001b[35m100\u001b[0m\n",
      "\u001b[2m2024-11-15 21:40:19\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mbatch ingest runbooks         \u001b[0m \u001b[36mbatch_size\u001b[0m=\u001b[35m61\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "runbook_table = get_runbooks_table()\n",
    "runbook_table.delete(\"1 = 1\")\n",
    "\n",
    "ingestion = DocumentIngestion(\n",
    "    metadata=Metadata(\n",
    "        name=\"k8s-concepts\",\n",
    "        description=\"Kubernetes Concepts\",\n",
    "    ),\n",
    "    spec=DocumentIngestionSpec(local_path=\"./website/content/en/docs/concepts/workloads/**/*.md\"),\n",
    ")\n",
    "\n",
    "ingester = DocumentIngester()\n",
    "\n",
    "ingester.document_ingestion(ingestion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunbookChunk(uuid='67620104-81bb-4054-a69a-5df93a8635f0', heading='', content='---\\nreviewers:\\n- enisoc\\n- erictune\\n- foxish\\n- janetkuo\\n- kow3ns\\ntitle: DaemonSet\\napi_metadata:\\n- apiVersion: \"apps/v1\"\\nkind: \"DaemonSet\"\\ndescription: >-\\nA DaemonSet defines Pods that provide node-local facilities. These might be fundamental to the operation of your cluster, such as a networking helper tool, or be part of an add-on.\\ncontent_type: concept\\nweight: 40\\nhide_summary: true # Listed separately in section index\\n---  \\n<!-- overview -->  \\nA _DaemonSet_ ensures that all (or some) Nodes run a copy of a Pod.  As nodes are added to the\\ncluster, Pods are added to them.  As nodes are removed from the cluster, those Pods are garbage\\ncollected.  Deleting a DaemonSet will clean up the Pods it created.  \\nSome typical uses of a DaemonSet are:  \\n- running a cluster storage daemon on every node\\n- running a logs collection daemon on every node\\n- running a node monitoring daemon on every node  \\nIn a simple case, one DaemonSet, covering all nodes, would be used for each type of daemon.\\nA more complex setup might use multiple DaemonSets for a single type of daemon, but with\\ndifferent flags and/or different memory and cpu requests for different hardware types.  \\n<!-- body -->')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class RunbookChunk(BaseModel):\n",
    "    uuid: str\n",
    "    heading: str\n",
    "    content: str\n",
    "\n",
    "runbook_table = get_runbooks_table()\n",
    "\n",
    "sample_runbooks = runbook_table.to_pandas()\n",
    "\n",
    "sample_chunks = [\n",
    "    RunbookChunk(\n",
    "        uuid=row[\"uuid\"],\n",
    "        heading=row[\"heading\"],\n",
    "        content=row[\"content\"],\n",
    "    )\n",
    "    for idx, row in sample_runbooks.iterrows()\n",
    "]\n",
    "\n",
    "sample_chunks[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's generate some synthetic questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ChunkEval(question='What does a DaemonSet do in a Kubernetes cluster?', answer='A DaemonSet ensures that all (or some) Nodes run a copy of a Pod. As nodes are added to the cluster, Pods are added to them, and as nodes are removed, those Pods are garbage collected.', uuid='67620104-81bb-4054-a69a-5df93a8635f0', question_with_context='A user asked the following question:\\nQuestion: What does a DaemonSet do in a Kubernetes cluster?\\nThis is about the following runbook:\\nRunbook Title: \\nRunbook Content: ---\\nreviewers:\\n- enisoc\\n- erictune\\n- foxish\\n- janetkuo\\n- kow3ns\\ntitle: DaemonSet\\napi_metadata:\\n- apiVersion: \"apps/v1\"\\nkind: \"DaemonSet\"\\ndescription: >-\\nA DaemonSet defines Pods that provide node-local facilities. These might be fundamental to the operation of your cluster, such as a networking helper tool, or be part of an add-on.\\ncontent_type: concept\\nweight: 40\\nhide_summary: true # Listed separately in section index\\n---  \\n<!-- overview -->  \\nA _DaemonSet_ ensures that all (or some) Nodes run a copy of a Pod.  As nodes are added to the\\ncluster, Pods are added to them.  As nodes are removed from the cluster, those Pods are garbage\\ncollected.  Deleting a DaemonSet will clean up the Pods it created.  \\nSome typical uses of a DaemonSet are:  \\n- running a cluster storage daemon on every node\\n- running a logs collection daemon on every node\\n- running a node monitoring daemon on every node  \\nIn a simple case, one DaemonSet, covering all nodes, would be used for each type of daemon.\\nA more complex setup might use multiple DaemonSets for a single type of daemon, but with\\ndifferent flags and/or different memory and cpu requests for different hardware types.  \\n<!-- body -->\\n'),\n",
       " ChunkEval(question='Can I use multiple DaemonSets for the same type of daemon?', answer='Yes, a more complex setup might use multiple DaemonSets for a single type of daemon, but with different flags and/or different memory and CPU requests for different hardware types.', uuid='67620104-81bb-4054-a69a-5df93a8635f0', question_with_context='A user asked the following question:\\nQuestion: Can I use multiple DaemonSets for the same type of daemon?\\nThis is about the following runbook:\\nRunbook Title: \\nRunbook Content: ---\\nreviewers:\\n- enisoc\\n- erictune\\n- foxish\\n- janetkuo\\n- kow3ns\\ntitle: DaemonSet\\napi_metadata:\\n- apiVersion: \"apps/v1\"\\nkind: \"DaemonSet\"\\ndescription: >-\\nA DaemonSet defines Pods that provide node-local facilities. These might be fundamental to the operation of your cluster, such as a networking helper tool, or be part of an add-on.\\ncontent_type: concept\\nweight: 40\\nhide_summary: true # Listed separately in section index\\n---  \\n<!-- overview -->  \\nA _DaemonSet_ ensures that all (or some) Nodes run a copy of a Pod.  As nodes are added to the\\ncluster, Pods are added to them.  As nodes are removed from the cluster, those Pods are garbage\\ncollected.  Deleting a DaemonSet will clean up the Pods it created.  \\nSome typical uses of a DaemonSet are:  \\n- running a cluster storage daemon on every node\\n- running a logs collection daemon on every node\\n- running a node monitoring daemon on every node  \\nIn a simple case, one DaemonSet, covering all nodes, would be used for each type of daemon.\\nA more complex setup might use multiple DaemonSets for a single type of daemon, but with\\ndifferent flags and/or different memory and cpu requests for different hardware types.  \\n<!-- body -->\\n'),\n",
       " ChunkEval(question='What happens if I delete a DaemonSet?', answer='Deleting a DaemonSet will clean up the Pods it created.', uuid='67620104-81bb-4054-a69a-5df93a8635f0', question_with_context='A user asked the following question:\\nQuestion: What happens if I delete a DaemonSet?\\nThis is about the following runbook:\\nRunbook Title: \\nRunbook Content: ---\\nreviewers:\\n- enisoc\\n- erictune\\n- foxish\\n- janetkuo\\n- kow3ns\\ntitle: DaemonSet\\napi_metadata:\\n- apiVersion: \"apps/v1\"\\nkind: \"DaemonSet\"\\ndescription: >-\\nA DaemonSet defines Pods that provide node-local facilities. These might be fundamental to the operation of your cluster, such as a networking helper tool, or be part of an add-on.\\ncontent_type: concept\\nweight: 40\\nhide_summary: true # Listed separately in section index\\n---  \\n<!-- overview -->  \\nA _DaemonSet_ ensures that all (or some) Nodes run a copy of a Pod.  As nodes are added to the\\ncluster, Pods are added to them.  As nodes are removed from the cluster, those Pods are garbage\\ncollected.  Deleting a DaemonSet will clean up the Pods it created.  \\nSome typical uses of a DaemonSet are:  \\n- running a cluster storage daemon on every node\\n- running a logs collection daemon on every node\\n- running a node monitoring daemon on every node  \\nIn a simple case, one DaemonSet, covering all nodes, would be used for each type of daemon.\\nA more complex setup might use multiple DaemonSets for a single type of daemon, but with\\ndifferent flags and/or different memory and cpu requests for different hardware types.  \\n<!-- body -->\\n')]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import List\n",
    "import instructor\n",
    "from openai import AsyncOpenAI\n",
    "from pydantic import BaseModel\n",
    "\n",
    "client = instructor.from_openai(AsyncOpenAI())\n",
    "\n",
    "\n",
    "example_questions = [\n",
    "    \"How to create a ephemeral pod?\",\n",
    "    \"How can I make sure at least 50% replicas are always running for a deployment?\",\n",
    "]\n",
    "\n",
    "class QuestionAnswer(BaseModel):\n",
    "    question: str\n",
    "    answer: str\n",
    "\n",
    "class ChunkEval(QuestionAnswer):\n",
    "    uuid: str\n",
    "    question_with_context: str\n",
    "\n",
    "async def generate_evals(runbook: RunbookChunk, n_questions: int, example_questions: List[str]) -> List[ChunkEval]:\n",
    "    prompt = f\"\"\"\n",
    "Generate `{n_questions}` question-answer pairs about {runbook.heading}. The answer should primarily derived from the information in the runbook content.\n",
    "\n",
    "<content>\n",
    "{runbook.content}\n",
    "</content>\n",
    "\n",
    "Example questions:\n",
    "{\"\\n\".join('f - {q}' for q in example_questions)}\n",
    "\n",
    "Provide a concise and specific answer for each question.\n",
    "Do not use the exact example questions. Use them only as inspiration for the types of more specific questions to generate.\n",
    "Do not include answers that are not in the content.\n",
    "Questions should ask about how to do certain things and the answer should refer to how to do certain things based on the technical knowledge in the runbook.\n",
    "Answers should be based on the content.\n",
    "Stylistically, the questions should resemble what people would ask a RAG-based answer bot on a technical documentation website. So they can be a little informal, messy or scattered.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "    def make_context(question: str) -> str:\n",
    "        return f\"\"\"A user asked the following question:\n",
    "Question: {question}\n",
    "This is about the following runbook:\n",
    "Runbook Title: {runbook.heading}\n",
    "Runbook Content: {runbook.content}\n",
    "\"\"\"\n",
    "\n",
    "    try:\n",
    "        pairs = client.chat.completions.create_iterable(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            response_model=QuestionAnswer,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0.0,\n",
    "        )\n",
    "\n",
    "        return [\n",
    "            ChunkEval(\n",
    "                uuid=runbook.uuid,\n",
    "                heading=runbook.heading,\n",
    "                question=pair.question,\n",
    "                answer=pair.answer,\n",
    "                question_with_context=make_context(pair.question),\n",
    "            )\n",
    "            async for pair in pairs\n",
    "        ]\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating evals: {str(e)}\")\n",
    "        return []\n",
    "\n",
    "first_chunk_res = await generate_evals(sample_chunks[0], 3, example_questions)\n",
    "first_chunk_res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ChunkEval(question='What does a DaemonSet do in a Kubernetes cluster?', answer='A DaemonSet ensures that all (or some) Nodes run a copy of a Pod, adding Pods as nodes are added to the cluster and garbage collecting them as nodes are removed.', uuid='67620104-81bb-4054-a69a-5df93a8635f0', question_with_context='A user asked the following question:\\nQuestion: What does a DaemonSet do in a Kubernetes cluster?\\nThis is about the following runbook:\\nRunbook Title: \\nRunbook Content: ---\\nreviewers:\\n- enisoc\\n- erictune\\n- foxish\\n- janetkuo\\n- kow3ns\\ntitle: DaemonSet\\napi_metadata:\\n- apiVersion: \"apps/v1\"\\nkind: \"DaemonSet\"\\ndescription: >-\\nA DaemonSet defines Pods that provide node-local facilities. These might be fundamental to the operation of your cluster, such as a networking helper tool, or be part of an add-on.\\ncontent_type: concept\\nweight: 40\\nhide_summary: true # Listed separately in section index\\n---  \\n<!-- overview -->  \\nA _DaemonSet_ ensures that all (or some) Nodes run a copy of a Pod.  As nodes are added to the\\ncluster, Pods are added to them.  As nodes are removed from the cluster, those Pods are garbage\\ncollected.  Deleting a DaemonSet will clean up the Pods it created.  \\nSome typical uses of a DaemonSet are:  \\n- running a cluster storage daemon on every node\\n- running a logs collection daemon on every node\\n- running a node monitoring daemon on every node  \\nIn a simple case, one DaemonSet, covering all nodes, would be used for each type of daemon.\\nA more complex setup might use multiple DaemonSets for a single type of daemon, but with\\ndifferent flags and/or different memory and cpu requests for different hardware types.  \\n<!-- body -->\\n'),\n",
       " ChunkEval(question='Can I use multiple DaemonSets for the same type of daemon?', answer='Yes, a more complex setup might use multiple DaemonSets for a single type of daemon, with different flags and/or different memory and CPU requests for different hardware types.', uuid='67620104-81bb-4054-a69a-5df93a8635f0', question_with_context='A user asked the following question:\\nQuestion: Can I use multiple DaemonSets for the same type of daemon?\\nThis is about the following runbook:\\nRunbook Title: \\nRunbook Content: ---\\nreviewers:\\n- enisoc\\n- erictune\\n- foxish\\n- janetkuo\\n- kow3ns\\ntitle: DaemonSet\\napi_metadata:\\n- apiVersion: \"apps/v1\"\\nkind: \"DaemonSet\"\\ndescription: >-\\nA DaemonSet defines Pods that provide node-local facilities. These might be fundamental to the operation of your cluster, such as a networking helper tool, or be part of an add-on.\\ncontent_type: concept\\nweight: 40\\nhide_summary: true # Listed separately in section index\\n---  \\n<!-- overview -->  \\nA _DaemonSet_ ensures that all (or some) Nodes run a copy of a Pod.  As nodes are added to the\\ncluster, Pods are added to them.  As nodes are removed from the cluster, those Pods are garbage\\ncollected.  Deleting a DaemonSet will clean up the Pods it created.  \\nSome typical uses of a DaemonSet are:  \\n- running a cluster storage daemon on every node\\n- running a logs collection daemon on every node\\n- running a node monitoring daemon on every node  \\nIn a simple case, one DaemonSet, covering all nodes, would be used for each type of daemon.\\nA more complex setup might use multiple DaemonSets for a single type of daemon, but with\\ndifferent flags and/or different memory and cpu requests for different hardware types.  \\n<!-- body -->\\n'),\n",
       " ChunkEval(question='What are some typical uses for a DaemonSet?', answer='Typical uses of a DaemonSet include running a cluster storage daemon, a logs collection daemon, and a node monitoring daemon on every node.', uuid='67620104-81bb-4054-a69a-5df93a8635f0', question_with_context='A user asked the following question:\\nQuestion: What are some typical uses for a DaemonSet?\\nThis is about the following runbook:\\nRunbook Title: \\nRunbook Content: ---\\nreviewers:\\n- enisoc\\n- erictune\\n- foxish\\n- janetkuo\\n- kow3ns\\ntitle: DaemonSet\\napi_metadata:\\n- apiVersion: \"apps/v1\"\\nkind: \"DaemonSet\"\\ndescription: >-\\nA DaemonSet defines Pods that provide node-local facilities. These might be fundamental to the operation of your cluster, such as a networking helper tool, or be part of an add-on.\\ncontent_type: concept\\nweight: 40\\nhide_summary: true # Listed separately in section index\\n---  \\n<!-- overview -->  \\nA _DaemonSet_ ensures that all (or some) Nodes run a copy of a Pod.  As nodes are added to the\\ncluster, Pods are added to them.  As nodes are removed from the cluster, those Pods are garbage\\ncollected.  Deleting a DaemonSet will clean up the Pods it created.  \\nSome typical uses of a DaemonSet are:  \\n- running a cluster storage daemon on every node\\n- running a logs collection daemon on every node\\n- running a node monitoring daemon on every node  \\nIn a simple case, one DaemonSet, covering all nodes, would be used for each type of daemon.\\nA more complex setup might use multiple DaemonSets for a single type of daemon, but with\\ndifferent flags and/or different memory and cpu requests for different hardware types.  \\n<!-- body -->\\n')]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import asyncio\n",
    "\n",
    "class ChunkProcessingError(Exception):\n",
    "    pass\n",
    "\n",
    "async def process_chunk(chunk: RunbookChunk, n_questions: int, example_questions: List[str], semaphore: asyncio.Semaphore) -> List[ChunkEval]:\n",
    "    async with semaphore:\n",
    "        try:\n",
    "            return await generate_evals(chunk, n_questions, example_questions)\n",
    "        except Exception as e:\n",
    "            raise ChunkProcessingError(f\"Error processing chunk {chunk.uuid}: {str(e)}\") from e\n",
    "\n",
    "\n",
    "await process_chunk(sample_chunks[0], 3, example_questions, asyncio.Semaphore(1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's call `process_chunk` with all chunks to build the full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import structlog\n",
    "import random\n",
    "\n",
    "logger = structlog.get_logger()\n",
    "\n",
    "async def create_synthetic_dataset(\n",
    "    chunks: List[RunbookChunk],\n",
    "    n_questions: int,\n",
    "    example_questions: List[str],\n",
    "    max_workers: int = 10,\n",
    ") -> List[ChunkEval]:\n",
    "    semaphore = asyncio.Semaphore(max_workers)\n",
    "    tasks = [process_chunk(chunk, n_questions, example_questions, semaphore) for chunk in chunks]\n",
    "    results = await asyncio.gather(*tasks, return_exceptions=True)\n",
    "\n",
    "    dataset = []\n",
    "    for result in results:\n",
    "        if isinstance(result, ChunkProcessingError):\n",
    "            print(f\"Error processing chunk: {result}\")\n",
    "        elif isinstance(result, list):\n",
    "            dataset.extend(result)\n",
    "        else:\n",
    "            print(f\"Unknown result type: {type(result)}\")\n",
    "    return dataset\n",
    "\n",
    "def save_eval_data(dataset: List[ChunkEval], filename: str):\n",
    "    with open(filename, \"w\") as f:\n",
    "        json.dump([e.model_dump() for e in dataset], f, indent=2)\n",
    "\n",
    "def save_tf_data(dataset: List[ChunkEval], filename: str):\n",
    "    df = runbook_table.to_pandas()\n",
    "    with open(filename, \"w\") as f:\n",
    "        for chunk_eval in dataset:\n",
    "            content = chunk_eval.question\n",
    "            f.write(json.dumps({\n",
    "                \"query\": chunk_eval.question_with_context,\n",
    "                \"relevant_passages\": [content]\n",
    "            }) + \"\\n\")\n",
    "\n",
    "synthetic_dataset = await create_synthetic_dataset(sample_chunks, 3, example_questions)\n",
    "random.shuffle(synthetic_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "783 391 392\n",
      "\u001b[2m2024-11-15 21:46:11\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mSynthetic eval dataset saved  \u001b[0m \u001b[36mdataset_len\u001b[0m=\u001b[35m783\u001b[0m \u001b[36meval_len\u001b[0m=\u001b[35m391\u001b[0m \u001b[36mft_len\u001b[0m=\u001b[35m392\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "split_idx = len(synthetic_dataset) // 2\n",
    "eval_dataset = synthetic_dataset[:split_idx]\n",
    "ft_dataset = synthetic_dataset[split_idx:]\n",
    "\n",
    "print(len(synthetic_dataset), len(eval_dataset), len(ft_dataset))\n",
    "save_eval_data(eval_dataset, \"synthetic_eval_dataset.json\")\n",
    "save_tf_data(ft_dataset, \"synthetic_ft_dataset.jsonl\")\n",
    "\n",
    "logger.info(\"Synthetic eval dataset saved\",\n",
    "    dataset_len=len(synthetic_dataset),\n",
    "    eval_len=len(eval_dataset),\n",
    "    ft_len=len(ft_dataset),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ChunkEval(question='When does the Job controller add terminal conditions in Kubernetes v1.31 and later?', answer='In Kubernetes v1.31 and later, the Job controller adds the terminal conditions `Failed` or `Complete` only after all of the Job Pods are terminated.', uuid='7c22487c-7361-4574-b313-371d70083d04', question_with_context='A user asked the following question:\\nQuestion: When does the Job controller add terminal conditions in Kubernetes v1.31 and later?\\nThis is about the following runbook:\\nRunbook Title: Terminal Job conditions\\nRunbook Content: Job termination and cleanupTerminal Job conditionsA Job has two possible terminal states, each of which has a corresponding Job\\ncondition:\\n* Succeeded:  Job condition `Complete`\\n* Failed: Job condition `Failed`  \\nJobs fail for the following reasons:\\n- The number of Pod failures exceeded the specified `.spec.backoffLimit` in the Job\\nspecification. For details, see [Pod backoff failure policy](#pod-backoff-failure-policy).\\n- The Job runtime exceeded the specified `.spec.activeDeadlineSeconds`\\n- An indexed Job that used `.spec.backoffLimitPerIndex` has failed indexes.\\nFor details, see [Backoff limit per index](#backoff-limit-per-index).\\n- The number of failed indexes in the Job exceeded the specified\\n`spec.maxFailedIndexes`. For details, see [Backoff limit per index](#backoff-limit-per-index)\\n- A failed Pod matches a rule in `.spec.podFailurePolicy` that has the `FailJob`\\naction. For details about how Pod failure policy rules might affect failure\\nevaluation, see [Pod failure policy](#pod-failure-policy).  \\nJobs succeed for the following reasons:\\n- The number of succeeded Pods reached the specified `.spec.completions`\\n- The criteria specified in `.spec.successPolicy` are met. For details, see\\n[Success policy](#success-policy).  \\nIn Kubernetes v1.31 and later the Job controller delays the addition of the\\nterminal conditions,`Failed` or `Complete`, until all of the Job Pods are terminated.  \\nIn Kubernetes v1.30 and earlier, the Job controller added the `Complete` or the\\n`Failed` Job terminal conditions as soon as the Job termination process was\\ntriggered and all Pod finalizers were removed. However, some Pods would still\\nbe running or terminating at the moment that the terminal condition was added.  \\nIn Kubernetes v1.31 and later, the controller only adds the Job terminal conditions\\n_after_ all of the Pods are terminated. You can enable this behavior by using the\\n`JobManagedBy` or the `JobPodReplacementPolicy` (enabled by default)\\n[feature gates](/docs/reference/command-line-tools-reference/feature-gates/).\\n'),\n",
       " ChunkEval(question='How do I set the Pod Selector for a StatefulSet?', answer='You must set the `.spec.selector` field of a StatefulSet to match the labels of its `.spec.template.metadata.labels`.', uuid='5ad2f4f1-9448-402f-8c0d-7bbb9c95db82', question_with_context='A user asked the following question:\\nQuestion: How do I set the Pod Selector for a StatefulSet?\\nThis is about the following runbook:\\nRunbook Title: Pod Selector\\nRunbook Content: ComponentsPod SelectorYou must set the `.spec.selector` field of a StatefulSet to match the labels of its\\n`.spec.template.metadata.labels`. Failing to specify a matching Pod Selector will result in a\\nvalidation error during StatefulSet creation.\\n'),\n",
       " ChunkEval(question='Will my applications that need to run as root work with user namespaces activated?', answer=\"Yes, most applications that need to run as root but don't access other host namespaces or resources should continue to run fine without any changes needed if user namespaces is activated.\", uuid='7598d4f3-b249-4018-b9c3-ee61832205b4', question_with_context=\"A user asked the following question:\\nQuestion: Will my applications that need to run as root work with user namespaces activated?\\nThis is about the following runbook:\\nRunbook Title: Introduction\\nRunbook Content: IntroductionUser namespaces is a Linux feature that allows to map users in the container to\\ndifferent users in the host. Furthermore, the capabilities granted to a pod in\\na user namespace are valid only in the namespace and void outside of it.  \\nA pod can opt-in to use user namespaces by setting the `pod.spec.hostUsers` field\\nto `false`.  \\nThe kubelet will pick host UIDs/GIDs a pod is mapped to, and will do so in a way\\nto guarantee that no two pods on the same node use the same mapping.  \\nThe `runAsUser`, `runAsGroup`, `fsGroup`, etc. fields in the `pod.spec` always\\nrefer to the user inside the container.  \\nThe valid UIDs/GIDs when this feature is enabled is the range 0-65535. This\\napplies to files and processes (`runAsUser`, `runAsGroup`, etc.).  \\nFiles using a UID/GID outside this range will be seen as belonging to the\\noverflow ID, usually 65534 (configured in `/proc/sys/kernel/overflowuid` and\\n`/proc/sys/kernel/overflowgid`). However, it is not possible to modify those\\nfiles, even by running as the 65534 user/group.  \\nMost applications that need to run as root but don't access other host\\nnamespaces or resources, should continue to run fine without any changes needed\\nif user namespaces is activated.\\n\"),\n",
       " ChunkEval(question='Can I set the TTL for already finished Jobs?', answer='Yes, you can manually set the `.spec.ttlSecondsAfterFinished` field for existing, already finished Jobs to make them eligible for cleanup.', uuid='dacd1f0c-3d62-41d8-b508-7033d1771c37', question_with_context='A user asked the following question:\\nQuestion: Can I set the TTL for already finished Jobs?\\nThis is about the following runbook:\\nRunbook Title: Cleanup for finished Jobs\\nRunbook Content: Cleanup for finished JobsThe TTL-after-finished controller is only supported for Jobs. You can use this mechanism to clean\\nup finished Jobs (either `Complete` or `Failed`) automatically by specifying the\\n`.spec.ttlSecondsAfterFinished` field of a Job, as in this\\n[example](/docs/concepts/workloads/controllers/job/#clean-up-finished-jobs-automatically).  \\nThe TTL-after-finished controller assumes that a Job is eligible to be cleaned up\\nTTL seconds after the Job has finished. The timer starts once the\\nstatus condition of the Job changes to show that the Job is either `Complete` or `Failed`; once the TTL has\\nexpired, that Job becomes eligible for\\n[cascading](/docs/concepts/architecture/garbage-collection/#cascading-deletion) removal. When the\\nTTL-after-finished controller cleans up a job, it will delete it cascadingly, that is to say it will delete\\nits dependent objects together with it.  \\nKubernetes honors object lifecycle guarantees on the Job, such as waiting for\\n[finalizers](/docs/concepts/overview/working-with-objects/finalizers/).  \\nYou can set the TTL seconds at any time. Here are some examples for setting the\\n`.spec.ttlSecondsAfterFinished` field of a Job:  \\n* Specify this field in the Job manifest, so that a Job can be cleaned up\\nautomatically some time after it finishes.\\n* Manually set this field of existing, already finished Jobs, so that they become eligible\\nfor cleanup.\\n* Use a\\n[mutating admission webhook](/docs/reference/access-authn-authz/admission-controllers/#mutatingadmissionwebhook)\\nto set this field dynamically at Job creation time. Cluster administrators can\\nuse this to enforce a TTL policy for finished jobs.\\n* Use a\\n[mutating admission webhook](/docs/reference/access-authn-authz/admission-controllers/#mutatingadmissionwebhook)\\nto set this field dynamically after the Job has finished, and choose\\ndifferent TTL values based on job status, labels. For this case, the webhook needs\\nto detect changes to the `.status` of the Job and only set a TTL when the Job\\nis being marked as completed.\\n* Write your own controller to manage the cleanup TTL for Jobs that match a particular\\n{{< glossary_tooltip term_id=\"selector\" text=\"selector\" >}}.\\n'),\n",
       " ChunkEval(question='How can I learn about Pods in Kubernetes?', answer='You can learn about Pods by visiting the [Pods documentation](/docs/concepts/workloads/pods).', uuid='d6335d1a-f4b1-4ed2-8c7e-6e8bd67b8dd2', question_with_context='A user asked the following question:\\nQuestion: How can I learn about Pods in Kubernetes?\\nThis is about the following runbook:\\nRunbook Title: {{% heading \"whatsnext\" %}}\\nRunbook Content: {{% heading \"whatsnext\" %}}* Learn about [Pods](/docs/concepts/workloads/pods).\\n* Learn about [Deployment](/docs/concepts/workloads/controllers/deployment/), the replacement\\nfor ReplicationController.\\n* `ReplicationController` is part of the Kubernetes REST API.\\nRead the {{< api-reference page=\"workload-resources/replication-controller-v1\" >}}\\nobject definition to understand the API for replication controllers.\\n'),\n",
       " ChunkEval(question='How does the ReplicationController ensure the number of pods is correct?', answer='The ReplicationController ensures that the desired number of pods matches its label selector and are operational, excluding only terminated pods from its count.', uuid='dab3cd51-b68c-4b3b-b0a9-3a50f33d293d', question_with_context='A user asked the following question:\\nQuestion: How does the ReplicationController ensure the number of pods is correct?\\nThis is about the following runbook:\\nRunbook Title: Responsibilities of the ReplicationController\\nRunbook Content: Responsibilities of the ReplicationControllerThe ReplicationController ensures that the desired number of pods matches its label selector and are operational. Currently, only terminated pods are excluded from its count. In the future, [readiness](https://issue.k8s.io/620) and other information available from the system may be taken into account, we may add more controls over the replacement policy, and we plan to emit events that could be used by external clients to implement arbitrarily sophisticated replacement and/or scale-down policies.  \\nThe ReplicationController is forever constrained to this narrow responsibility. It itself will not perform readiness nor liveness probes. Rather than performing auto-scaling, it is intended to be controlled by an external auto-scaler (as discussed in [#492](https://issue.k8s.io/492)), which would change its `replicas` field. We will not add scheduling policies (for example, [spreading](https://issue.k8s.io/367#issuecomment-48428019)) to the ReplicationController. Nor should it verify that the pods controlled match the currently specified template, as that would obstruct auto-sizing and other automated processes. Similarly, completion deadlines, ordering dependencies, configuration expansion, and other features belong elsewhere. We even plan to factor out the mechanism for bulk pod creation ([#170](https://issue.k8s.io/170)).  \\nThe ReplicationController is intended to be a composable building-block primitive. We expect higher-level APIs and/or tools to be built on top of it and other complementary primitives for user convenience in the future. The \"macro\" operations currently supported by kubectl (run, scale) are proof-of-concept examples of this. For instance, we could imagine something like [Asgard](https://netflixtechblog.com/asgard-web-based-cloud-management-and-deployment-2c9fc4e4d3a1) managing ReplicationControllers, auto-scalers, services, scheduling policies, canaries, etc.\\n'),\n",
       " ChunkEval(question='How should I handle labels in the Pod Template?', answer='When specifying labels in the Pod Template, make sure they do not overlap with labels used by other controllers.', uuid='79d0fde0-3a05-4c97-b7bc-4789e1110272', question_with_context='A user asked the following question:\\nQuestion: How should I handle labels in the Pod Template?\\nThis is about the following runbook:\\nRunbook Title: Pod Template\\nRunbook Content: Writing a Deployment SpecPod TemplateThe `.spec.template` and `.spec.selector` are the only required fields of the `.spec`.  \\nThe `.spec.template` is a [Pod template](/docs/concepts/workloads/pods/#pod-templates). It has exactly the same schema as a {{< glossary_tooltip text=\"Pod\" term_id=\"pod\" >}}, except it is nested and does not have an `apiVersion` or `kind`.  \\nIn addition to required fields for a Pod, a Pod template in a Deployment must specify appropriate\\nlabels and an appropriate restart policy. For labels, make sure not to overlap with other controllers. See [selector](#selector).  \\nOnly a [`.spec.template.spec.restartPolicy`](/docs/concepts/workloads/pods/pod-lifecycle/#restart-policy) equal to `Always` is\\nallowed, which is the default if not specified.\\n'),\n",
       " ChunkEval(question='Can I prevent involuntary disruptions with a PodDisruptionBudget?', answer='No, involuntary disruptions cannot be prevented by PodDisruptionBudgets; however, they do count against the budget.', uuid='406c8f76-ccbd-4a8a-a0a6-ebce8ce90ef2', question_with_context='A user asked the following question:\\nQuestion: Can I prevent involuntary disruptions with a PodDisruptionBudget?\\nThis is about the following runbook:\\nRunbook Title: Pod disruption budgets\\nRunbook Content: Pod disruption budgets{{< feature-state for_k8s_version=\"v1.21\" state=\"stable\" >}}  \\nKubernetes offers features to help you run highly available applications even when you\\nintroduce frequent voluntary disruptions.  \\nAs an application owner, you can create a PodDisruptionBudget (PDB) for each application.\\nA PDB limits the number of Pods of a replicated application that are down simultaneously from\\nvoluntary disruptions. For example, a quorum-based application would\\nlike to ensure that the number of replicas running is never brought below the\\nnumber needed for a quorum. A web front end might want to\\nensure that the number of replicas serving load never falls below a certain\\npercentage of the total.  \\nCluster managers and hosting providers should use tools which\\nrespect PodDisruptionBudgets by calling the [Eviction API](/docs/tasks/administer-cluster/safely-drain-node/#eviction-api)\\ninstead of directly deleting pods or deployments.  \\nFor example, the `kubectl drain` subcommand lets you mark a node as going out of\\nservice. When you run `kubectl drain`, the tool tries to evict all of the Pods on\\nthe Node you\\'re taking out of service. The eviction request that `kubectl` submits on\\nyour behalf may be temporarily rejected, so the tool periodically retries all failed\\nrequests until all Pods on the target node are terminated, or until a configurable timeout\\nis reached.  \\nA PDB specifies the number of replicas that an application can tolerate having, relative to how\\nmany it is intended to have.  For example, a Deployment which has a `.spec.replicas: 5` is\\nsupposed to have 5 pods at any given time.  If its PDB allows for there to be 4 at a time,\\nthen the Eviction API will allow voluntary disruption of one (but not two) pods at a time.  \\nThe group of pods that comprise the application is specified using a label selector, the same\\nas the one used by the application\\'s controller (deployment, stateful-set, etc).  \\nThe \"intended\" number of pods is computed from the `.spec.replicas` of the workload resource\\nthat is managing those pods. The control plane discovers the owning workload resource by\\nexamining the `.metadata.ownerReferences` of the Pod.  \\n[Involuntary disruptions](#voluntary-and-involuntary-disruptions) cannot be prevented by PDBs; however they\\ndo count against the budget.  \\nPods which are deleted or unavailable due to a rolling upgrade to an application do count\\nagainst the disruption budget, but workload resources (such as Deployment and StatefulSet)\\nare not limited by PDBs when doing rolling upgrades. Instead, the handling of failures\\nduring application updates is configured in the spec for the specific workload resource.  \\nIt is recommended to set `AlwaysAllow` [Unhealthy Pod Eviction Policy](/docs/tasks/run-application/configure-pdb/#unhealthy-pod-eviction-policy)\\nto your PodDisruptionBudgets to support eviction of misbehaving applications during a node drain.\\nThe default behavior is to wait for the application pods to become [healthy](/docs/tasks/run-application/configure-pdb/#healthiness-of-a-pod)\\nbefore the drain can proceed.  \\nWhen a pod is evicted using the eviction API, it is gracefully\\n[terminated](/docs/concepts/workloads/pods/pod-lifecycle/#pod-termination), honoring the\\n`terminationGracePeriodSeconds` setting in its [PodSpec](/docs/reference/generated/kubernetes-api/{{< param \"version\" >}}/#podspec-v1-core).\\n'),\n",
       " ChunkEval(question='Why should I use a ReplicationController even for a single pod application?', answer='You should use a ReplicationController because it ensures that your pod is automatically re-created on a node after disruptions, such as maintenance.', uuid='c9f7a9ca-2485-4edb-848e-3b8a868e4a72', question_with_context='A user asked the following question:\\nQuestion: Why should I use a ReplicationController even for a single pod application?\\nThis is about the following runbook:\\nRunbook Title: How a ReplicationController works\\nRunbook Content: How a ReplicationController worksIf there are too many pods, the ReplicationController terminates the extra pods. If there are too few, the\\nReplicationController starts more pods. Unlike manually created pods, the pods maintained by a\\nReplicationController are automatically replaced if they fail, are deleted, or are terminated.\\nFor example, your pods are re-created on a node after disruptive maintenance such as a kernel upgrade.\\nFor this reason, you should use a ReplicationController even if your application requires\\nonly a single pod. A ReplicationController is similar to a process supervisor,\\nbut instead of supervising individual processes on a single node, the ReplicationController supervises multiple pods\\nacross multiple nodes.  \\nReplicationController is often abbreviated to \"rc\" in discussion, and as a shortcut in\\nkubectl commands.  \\nA simple case is to create one ReplicationController object to reliably run one instance of\\na Pod indefinitely.  A more complex use case is to run several identical replicas of a replicated\\nservice, such as web servers.\\n'),\n",
       " ChunkEval(question=\"If I want to update my pods to a new spec, what's the best way to do it?\", answer='To update pods to a new spec in a controlled way, you should use a rolling update.', uuid='4dec0fe8-2314-4cc2-a958-3091f5cb06af', question_with_context=\"A user asked the following question:\\nQuestion: If I want to update my pods to a new spec, what's the best way to do it?\\nThis is about the following runbook:\\nRunbook Title: Deleting only a ReplicationController\\nRunbook Content: Working with ReplicationControllersDeleting only a ReplicationControllerYou can delete a ReplicationController without affecting any of its pods.  \\nUsing kubectl, specify the `--cascade=orphan` option to [`kubectl delete`](/docs/reference/generated/kubectl/kubectl-commands#delete).  \\nWhen using the REST API or [client library](/docs/reference/using-api/client-libraries), you can delete the ReplicationController object.  \\nOnce the original is deleted, you can create a new ReplicationController to replace it.  As long\\nas the old and new `.spec.selector` are the same, then the new one will adopt the old pods.\\nHowever, it will not make any effort to make existing pods match a new, different pod template.\\nTo update pods to a new spec in a controlled way, use a [rolling update](#rolling-updates).\\n\")]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"synthetic_eval_dataset.json\", \"r\") as f:\n",
    "    eval_dataset = json.load(f)\n",
    "\n",
    "eval_dataset_sample = eval_dataset[:10]\n",
    "\n",
    "eval_questions = [ChunkEval(**e) for e in eval_dataset_sample]\n",
    "\n",
    "eval_questions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_simple_request(q: ChunkEval, n_return_vals=5):\n",
    "    results = (\n",
    "        runbook_table.search(q.question_with_context).select([\"uuid\"]).limit(n_return_vals).to_list()\n",
    "    )\n",
    "    return [str(q.uuid) == str(r[\"uuid\"]) for r in results]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(hits):\n",
    "    n_retrieval_requests = len(hits)\n",
    "    total_retrievals = sum(len(l) for l in hits)\n",
    "    true_positives = sum(sum(sublist) for sublist in hits)\n",
    "\n",
    "    logger.info(\"Score\", n_retrieval_requests=n_retrieval_requests, total_retrievals=total_retrievals, true_positives=true_positives)\n",
    "    precision = true_positives / total_retrievals if total_retrievals > 0 else 0\n",
    "    recall = true_positives / n_retrieval_requests if n_retrieval_requests > 0 else 0\n",
    "    return {\"precision\": precision, \"recall\": recall}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[True, False, False, False, False], [True, False, False, False, False], [True, False, False, False, False], [True, False, False, False, False], [True, False, False, False, False], [True, False, False, False, False], [True, False, False, False, False], [True, False, False, False, False], [True, False, False, False, False], [True, False, False, False, False]]\n",
      "\u001b[2m2024-11-15 21:47:31\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mScore                         \u001b[0m \u001b[36mn_retrieval_requests\u001b[0m=\u001b[35m10\u001b[0m \u001b[36mtotal_retrievals\u001b[0m=\u001b[35m50\u001b[0m \u001b[36mtrue_positives\u001b[0m=\u001b[35m10\u001b[0m\n",
      "[[True, False, False, False, False, False, False, False, False, False], [True, False, False, False, False, False, False, False, False, False], [True, False, False, False, False, False, False, False, False, False], [True, False, False, False, False, False, False, False, False, False], [True, False, False, False, False, False, False, False, False, False], [True, False, False, False, False, False, False, False, False, False], [True, False, False, False, False, False, False, False, False, False], [True, False, False, False, False, False, False, False, False, False], [True, False, False, False, False, False, False, False, False, False], [True, False, False, False, False, False, False, False, False, False]]\n",
      "\u001b[2m2024-11-15 21:47:32\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mScore                         \u001b[0m \u001b[36mn_retrieval_requests\u001b[0m=\u001b[35m10\u001b[0m \u001b[36mtotal_retrievals\u001b[0m=\u001b[35m100\u001b[0m \u001b[36mtrue_positives\u001b[0m=\u001b[35m10\u001b[0m\n",
      "[[True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False], [True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False], [True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False], [True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False], [True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False], [True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False], [True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False], [True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False], [True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False], [True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False]]\n",
      "\u001b[2m2024-11-15 21:47:32\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mScore                         \u001b[0m \u001b[36mn_retrieval_requests\u001b[0m=\u001b[35m10\u001b[0m \u001b[36mtotal_retrievals\u001b[0m=\u001b[35m200\u001b[0m \u001b[36mtrue_positives\u001b[0m=\u001b[35m10\u001b[0m\n",
      "[[True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False], [True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False], [True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False], [True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False], [True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False], [True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False], [True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False], [True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False], [True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False], [True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False]]\n",
      "\u001b[2m2024-11-15 21:47:33\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mScore                         \u001b[0m \u001b[36mn_retrieval_requests\u001b[0m=\u001b[35m10\u001b[0m \u001b[36mtotal_retrievals\u001b[0m=\u001b[35m1000\u001b[0m \u001b[36mtrue_positives\u001b[0m=\u001b[35m10\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>n_retrieved</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.20</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.10</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.05</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.01</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   precision  recall  n_retrieved\n",
       "0       0.20     1.0            5\n",
       "1       0.10     1.0           10\n",
       "2       0.05     1.0           20\n",
       "3       0.01     1.0          100"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from typing import List, Dict\n",
    "\n",
    "def score_simple_search(n_to_retrieve: int) -> Dict[str, float]:\n",
    "    # parallelize to speed this up 5-10X\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        hits = list(\n",
    "            executor.map(lambda q: run_simple_request(q, n_to_retrieve), eval_questions)\n",
    "        )\n",
    "    return score(hits)\n",
    "\n",
    "k_to_retrieve = [5, 10, 20, 100]\n",
    "scores = pd.DataFrame([score_simple_search(n) for n in k_to_retrieve])\n",
    "scores[\"n_retrieved\"] = k_to_retrieve\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
