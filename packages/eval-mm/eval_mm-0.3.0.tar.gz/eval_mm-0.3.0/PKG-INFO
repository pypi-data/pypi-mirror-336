Metadata-Version: 2.4
Name: eval-mm
Version: 0.3.0
Summary: eval-mm is a tool for evaluating Multi-Modal Large Language Models.
Project-URL: Repository, https://github.com/llm-jp/llm-jp-eval-mm
Author-email: Silviase <koki.maeda@nlp.c.titech.ac.jp>, speed1313 <sugiura.issa.q29@kyoto-u.jp>
License-Expression: Apache-2.0
License-File: LICENSE
Requires-Python: >=3.12.1
Requires-Dist: av>=14.1.0
Requires-Dist: backoff>=2.2.1
Requires-Dist: datasets==2.18.0
Requires-Dist: emoji>=2.12.1
Requires-Dist: fugashi>=1.3.2
Requires-Dist: loguru>=0.7.3
Requires-Dist: openai>=1.42.0
Requires-Dist: protobuf>=5.29.1
Requires-Dist: python-dotenv>=1.0.1
Requires-Dist: requests>=2.32.3
Requires-Dist: rouge-score>=0.1.2
Requires-Dist: sacrebleu[ja]>=2.4.3
Requires-Dist: scipy>=1.15.1
Requires-Dist: torch>=2.5.1
Requires-Dist: unidic-lite>=1.0.8
Requires-Dist: webdataset>=0.2.111
Description-Content-Type: text/markdown

# llm-jp-eval-mm
[![pypi](https://img.shields.io/pypi/v/eval-mm.svg)](https://pypi.python.org/pypi/eval-mm) [![Test workflow](https://github.com/llm-jp/llm-jp-eval-mm/actions/workflows/test.yml/badge.svg)](https://github.com/llm-jp/llm-jp-eval-mm/actions/workflows/test.yml) [![License](https://img.shields.io/badge/License-Apache_2.0-blue.svg)](https://opensource.org/licenses/Apache-2.0)

[ [**Japanese**](./README_ja.md) | English ]

This tool automatically evaluates Japanese multi-modal large language models across multiple datasets. It offers the following features:

- Uses existing Japanese evaluation data and converts it into multi-modal text generation tasks for evaluation.
- Calculates task-specific evaluation metrics using inference results created by users.

![What llm-jp-eval-mm provides](https://github.com/llm-jp/llm-jp-eval-mm/blob/master/assets/teaser.png)

## Table of Contents

- [llm-jp-eval-mm](#llm-jp-eval-mm)
  - [Table of Contents](#table-of-contents)
  - [Getting Started](#getting-started)
  - [How to Evaluate](#how-to-evaluate)
    - [Running an Evaluation](#running-an-evaluation)
    - [Leaderboard](#leaderboard)
  - [Supported Tasks](#supported-tasks)
  - [Required Libraries for Each VLM Model Inference](#required-libraries-for-each-vlm-model-inference)
  - [Benchmark-Specific Required Libraries](#benchmark-specific-required-libraries)
  - [Analyze VLMs Prediction](#analyze-vlms-prediction)
  - [License](#license)
  - [Contribution](#contribution)
    - [How to Add a Benchmark Task](#how-to-add-a-benchmark-task)
    - [How to Add a Metric](#how-to-add-a-metric)
    - [How to Add Inference Code for a VLM Model](#how-to-add-inference-code-for-a-vlm-model)
    - [How to Add Dependencies](#how-to-add-dependencies)
    - [Testing](#testing)
    - [Formatting and Linting with ruff](#formatting-and-linting-with-ruff)
    - [How to Release to PyPI](#how-to-release-to-pypi)
    - [How to Update the Website](#how-to-update-the-website)
  - [Acknowledgements](#acknowledgements)

## Getting Started

You can use this tool via GitHub (Recommended).

```bash
git clone git@github.com:llm-jp/llm-jp-eval-mm.git
cd llm-jp-eval-mm
uv sync
```

Or you can install it via PyPI.
```bash
pip install eval_mm
```

This tool uses the LLM-as-a-judge method for evaluation, which sends requests to GPT-4o via the OpenAI API. Please create a `.env` file and set `AZURE_OPENAI_ENDPOINT` and `AZURE_OPENAI_KEY` if you’re using Azure, or `OPENAI_API_KEY` if you’re using the OpenAI API.

That’s it! You’re ready to evaluate your VLM model.

## How to Evaluate

### Running an Evaluation

We provide a sample code `examples/sample.py` for running an evaluation.

Models listed as `examples/{model_name}.py` are supported only in terms of their inference method.

If you want to run an evaluation on a new inference method or a new model, create a similar file referencing existing `examples/{model_name}.py`, and you can run the evaluation in the same way.

For example, if you want to evaluate the `llava-hf/llava-1.5-7b-hf` model on japanese-heron-bench task, run the following command:

```bash
uv sync --group normal
uv run --group normal python examples/sample.py \
  --model_id llava-hf/llava-1.5-7b-hf \
  --task_id japanese-heron-bench  \
  --result_dir result  \
  --metrics "heron-bench" \
  --judge_model "gpt-4o-2024-05-13" \
  --overwrite
```

The evaluation score and model outputs will be saved in the `result` directory like below:
```
├── japanese-heron-bench
│   ├── llava-hf
│   │   ├── llava-1.5-7b-hf
│   │   │   ├── evaluation.jsonl
│   │   │   └── prediction.jsonl
```

If you want to evaluate multiple models on multiple tasks, please check `eval_all.sh`.

### Leaderboard

You can create a leaderboard.md file by running the following command:
```bash
python scripts/make_leaderboard.py --result_dir result
```

Table like below will be created in `leaderboard.md` file.
| Model                    | Heron/LLM | JVB-ItW/LLM | JVB-ItW/Rouge |
| :----------------------- | --------: | ----------: | ------------: |
| llava-hf/llava-1.5-7b-hf |   36.9038 |         2.7 |       40.7525 |



Official Leaderboard is [here](https://llm-jp.github.io/llm-jp-eval-mm/)

## Supported Tasks

Right now, the following benchmark tasks are supported:

Japanese Task:
- [Japanese Heron Bench](https://huggingface.co/datasets/turing-motors/Japanese-Heron-Bench)
- [JA-VG-VQA500](https://huggingface.co/datasets/SakanaAI/JA-VG-VQA-500)
- [JA-VLM-Bench-In-the-Wild](https://huggingface.co/datasets/SakanaAI/JA-VLM-Bench-In-the-Wild)
- [JA-Multi-Image-VQA](https://huggingface.co/datasets/SakanaAI/JA-Multi-Image-VQA)
- [JDocQA](https://github.com/mizuumi/JDocQA)
- [JMMMU](https://huggingface.co/datasets/JMMMU/JMMMU)
- [JIC-VQA](https://huggingface.co/datasets/line-corporation/JIC-VQA)
- [MECHA-ja](https://huggingface.co/datasets/llm-jp/MECHA-ja)

English Task:
- [MMMU](https://huggingface.co/datasets/MMMU/MMMU)
- [LlaVA-Bench-In-the-Wild](https://huggingface.co/datasets/lmms-lab/llava-bench-in-the-wild)

## Required Libraries for Each VLM Model Inference

Different models require different libraries.
In this repository, we use uv’s [Dependency groups](https://docs.astral.sh/uv/concepts/projects/dependencies/#dependency-groups) to manage the libraries needed for each model.

For example, when you use `llm-jp/llm-jp-3-vila-14b`, please specify the `vilaja` group:
```bash
uv sync --group vilaja
uv run --group vilaja python examples/VILA_ja.py
```

For other models, please see the `eval_all.sh` script for the required group.

When you add a new group, don’t forget to configure [conflict](https://docs.astral.sh/uv/concepts/projects/config/#conflicting-dependencies).

## Benchmark-Specific Required Libraries

- JIC-VQA

JIC-VQA only provide the image URL, so you need to download the images from the URL. You can use the following code to prepare the JIC-VQA dataset with the image download.

```python
python scripts/prepare_jic_vqa.py
```

## Analyze VLMs Prediction

Let's analyze VLMs prediction!
```bash
uv run streamlit run scripts/browse_prediction.py --task_id "japanese-heron-bench" --result_dir "result"
```
You can see the visualization like below.
![Streamlit](./assets/streamlit_visualization.png)


## License

This repository is licensed under the Apache-2.0 License.

## Contribution

- If you find any issues or have suggestions, please report them on the Issue.
- If you add new benchmark tasks, metrics, or VLM model inference code, or if you fix bugs, please send us a Pull Request.

### How to Add a Benchmark Task
Tasks are defined in the `Task` class.
Please reference the code in [src/eval_mm/tasks](https://github.com/llm-jp/llm-jp-eval-mm/blob/master/src/eval_mm/tasks) and implement your `Task` class. You’ll need methods to convert the dataset into a format for input to the VLM model, and methods to calculate the score.

### How to Add a Metric
Metrics are defined in the `Scorer` class.
Please reference the code in [src/eval_mm/metrics](https://github.com/llm-jp/llm-jp-eval-mm/blob/master/src/eval_mm/metrics) and implement your `Scorer` class. You’ll need to implement a `score()` method for sample-level scoring comparing references and generated outputs, and an `aggregate()` method for population-level metric calculation.

### How to Add Inference Code for a VLM Model
Inference code for VLM models is defined in the `VLM` class.
Please reference [examples/base_vlm](https://github.com/llm-jp/llm-jp-eval-mm/blob/master/examples/base_vlm.py) and implement your `VLM` class. You’ll need a `generate()` method to output text given images and text inputs.

### How to Add Dependencies

```
uv add <package_name>
uv add --group <group_name> <package_name>
```


### Testing

You can test task classes and metric classes with the following command:
```bash
bash test.sh
```
You can also test each model's inference code with the following command:
```bash
bash test_model.sh
```

### Formatting and Linting with ruff
```
uv run ruff format src
uv run ruff check --fix src
```

### How to Release to PyPI

```
git tag -a v0.x.x -m "version 0.x.x"
git push origin --tags
```
Or you can manually create a new release on GitHub.


### How to Update the Website
Please refer to [github_pages/README.md](./github_pages/README.md).

## Acknowledgements
- [Heron](https://github.com/turingmotors/heron): We refer to the Heron code for the evaluation of the Japanese Heron Bench task.
- [lmms-eval](https://github.com/EvolvingLMMs-Lab/lmms-eval): We refer to the lmms-eval code for the evaluation of the JMMMU and MMMU tasks.

We also thank the developers of the evaluation datasets for their hard work.
