{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tutorial: MPI-parallelized calculation of spectra\n",
    "=================\n",
    "*01/2021: updated to pyGDM v1.1+*\n",
    "\n",
    "In this short tutorial we demonstrate how to accelerate the calculation of spectra via MPI multi-processing parallelization.\n",
    "\n",
    "Load modules\n",
    "-------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## --- for benchmark: limit openmp to 1 thread (for parallel scipy/numpy routines)\n",
    "##     Must be done before loading numpy to have an effect\n",
    "\n",
    "import os\n",
    "nthreads = 1\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"{}\".format(int(nthreads))\n",
    "os.environ[\"NUMEXPR_NUM_THREADS\"] = \"{}\".format(int(nthreads))\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"{}\".format(int(nthreads))\n",
    "\n",
    "## --- load pyGDM modules\n",
    "from pyGDM2 import structures\n",
    "from pyGDM2 import materials\n",
    "from pyGDM2 import fields\n",
    "from pyGDM2 import propagators\n",
    "\n",
    "from pyGDM2 import core\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "## --- Note: It is not necessary to load mpi4py within the simulation script, this \n",
    "## --- will be done automatically by pyGDM2 prior to the actual MPI-simulation. \n",
    "## --- We do it however at this point to do some output to stdout only from \n",
    "## --- the master process (rank == 0).\n",
    "from mpi4py import MPI\n",
    "rank = MPI.COMM_WORLD.rank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Config of the simulation\n",
    "--------------------------------------\n",
    "\n",
    "We will demonstrate the MPI spectra calculation on the simple example of a small dielectric sphere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "structure initialization - automatic mesh detection: cube\n",
      "structure initialization - consistency check: 203/203 dipoles valid\n"
     ]
    }
   ],
   "source": [
    "## ---------- Setup structure\n",
    "mesh = 'cube'\n",
    "step = 20.0\n",
    "radius = 3.5\n",
    "geometry = structures.sphere(step, R=radius, mesh=mesh)\n",
    "material = materials.dummy(2.0)\n",
    "struct = structures.struct(step, geometry, material)\n",
    "\n",
    "## ---------- Setup incident field\n",
    "field_generator = fields.planewave\n",
    "wavelengths = np.linspace(400, 800, 20)\n",
    "kwargs = dict(theta = [0.0])\n",
    "efield = fields.efield(field_generator, wavelengths=wavelengths, kwargs=kwargs)\n",
    "\n",
    "## ---------- vacuum environment\n",
    "n1, n2 = 1.0, 1.0\n",
    "dyads = propagators.DyadsQuasistatic123(n1=n1, n2=n2)\n",
    "\n",
    "## ---------- Simulation initialization\n",
    "sim = core.simulation(struct, efield, dyads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the simulation with the MPI wrapper to *core.scatter*\n",
    "---------------------------------------------\n",
    "\n",
    "The only difference to a non MPI-parallelized run of the simulation is, that we use *core.scatter_mpi* instead of *core.scatter*. *scatter_mpi* will automatically distribute the calculation of the different wavelengths in the spectrum on the available processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "performing MPI parallel simulation... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hans/.local/lib/python3.8/site-packages/pyGDM2/core.py:443: UserWarning: Executing only one MPI process! Should be run using e.g. 'mpirun -n X python scriptname.py', where X is the number of parallel processes.\n",
      "  warnings.warn(\"Executing only one MPI process! Should be run using\" +\n",
      "/home/hans/.local/lib/python3.8/site-packages/numba/core/dispatcher.py:237: UserWarning: Numba extension module 'numba_scipy' failed to load due to 'ValueError(No function '__pyx_fuse_0pdtr' found in __pyx_capi__ of 'scipy.special.cython_special')'.\n",
      "  entrypoints.init_all()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "simulation done.\n"
     ]
    }
   ],
   "source": [
    "## --- mpi: print in process with rank=0, to avoid flooding of stdout\n",
    "if rank == 0: \n",
    "    print(\"performing MPI parallel simulation... \")\n",
    "\n",
    "core.scatter_mpi(sim)\n",
    "\n",
    "if rank == 0: \n",
    "    print(\"simulation done.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IMPORTANT NOTE:** In order to be run by MPI, the script needs to be executed by the program **mpirun**:\n",
    "\n",
    "    $ mpirun -n 4 python3 pygdm_script_using_mpi.py\n",
    "\n",
    "where in this example, the argument \"-n 4\" tells MPI to run 4 parallel processes.\n",
    "\n",
    "**Note**: in case the number of wavelengths in the spectra is not divisable by the number of MPI processes, some MPI-processes will be idle for some time during execution. In this case *scatter_mpi* will return a warning:\n",
    "\n",
    "    UserWarning: Efficiency warning: Number of wavelengths (20) not divisable by Nr of processes (3)!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Timing of the above example\n",
    "--------------------\n",
    " \n",
    " - MPI-run:         1.25 s\n",
    " - sequential-run:  4.05 s\n",
    " - speed-up:        x3.2\n",
    " \n",
    "This should be more close to x4 in case of a larger simulation, where the MPI overhead becomes small compared to the simulation runtime."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
