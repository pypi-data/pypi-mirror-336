Metadata-Version: 2.4
Name: toxic_comment_classifier
Version: 0.2.1
Summary: A Python library for classifying toxic comments using deep learning.
Author-email: Md Irfan Ali <irfanali29@hotmail.com>
License: MIT
Project-URL: Homepage, https://github.com/irfanalidv/toxic_comment_classifier
Project-URL: Repository, https://github.com/irfanalidv/toxic_comment_classifier
Project-URL: Issues, https://github.com/irfanalidv/toxic_comment_classifier/issues
Classifier: Programming Language :: Python :: 3
Classifier: License :: OSI Approved :: MIT License
Classifier: Operating System :: OS Independent
Requires-Python: >=3.6
Description-Content-Type: text/markdown
Requires-Dist: numpy
Requires-Dist: tensorflow
Requires-Dist: pandas
Requires-Dist: scikit-learn

````markdown
# Toxic Comment Classifier

A Python library for classifying toxic comments using deep learning.
It supports detecting multiple types of toxicity including obscene language, threats, and identity hate.


````
## ðŸ“¦ Installation

```python
pip install toxic-comment-classifier
```
---

## ðŸš€ Usage

### ðŸ”¹ Import and Initialize the Model

```python
from toxic_classifier.model import ToxicCommentClassifier

# Load the classifier
model = ToxicCommentClassifier()
```

---

### ðŸ”¹ Classify a Single Comment

```python
text = "You are so dumb and stupid!"
scores = model.classify(text)

print("Toxicity Scores:", scores)
```

**Example Output:**

```python
{
    'toxic': 0.5004,
    'severe_toxic': 0.4987,
    'obscene': 0.4989,
    'threat': 0.5021,
    'insult': 0.4979,
    'identity_hate': 0.5006
}
```

---

### ðŸ”¹ Get Overall Toxicity Score

```python
toxicity = model.predict(text)
print(f"Overall Toxicity Score: {toxicity:.4f}")
```

**Example Output:**

```python
Overall Toxicity Score: 0.4998
```

---

### ðŸ”¹ Classify Multiple Comments

```python
texts = [
    "I hate this!",
    "You're amazing!",
    "This is the worst thing ever!"
]

scores = model.predict_batch(texts)

for txt, score in zip(texts, scores):
    print(f"Text: {txt} --> Toxicity Score: {score:.4f}")
```

**Example Output:**

```python
Text: I hate this! --> Toxicity Score: 0.5002
Text: You're amazing! --> Toxicity Score: 0.5000
Text: This is the worst thing ever! --> Toxicity Score: 0.5008
```

---

