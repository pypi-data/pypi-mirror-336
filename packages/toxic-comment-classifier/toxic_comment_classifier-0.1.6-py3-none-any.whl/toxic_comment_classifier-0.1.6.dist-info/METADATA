Metadata-Version: 2.4
Name: toxic_comment_classifier
Version: 0.1.6
Summary: A Python library for classifying toxic comments using deep learning.
Author-email: Md Irfan Ali <irfanali29@hotmail.com>
License: MIT
Classifier: Programming Language :: Python :: 3
Classifier: License :: OSI Approved :: MIT License
Classifier: Operating System :: OS Independent
Requires-Python: >=3.6
Description-Content-Type: text/markdown
Requires-Dist: numpy
Requires-Dist: tensorflow
Requires-Dist: pandas
Requires-Dist: scikit-learn

---

```markdown
# Toxic Comment Classifier

A Python library for classifying toxic comments using deep learning. It supports detecting multiple types of toxicity including obscene language, threats, and identity hate.

---

## ðŸ“¦ Installation

```python
pip install toxic-comment-classifier

```

---

## ðŸš€ Usage

### ðŸ”¹ Import and Initialize the Model

```python
from toxic_classifier.model import ToxicCommentClassifier

# Load the classifier
model = ToxicCommentClassifier()
```

---

### ðŸ”¹ Classify a Single Comment

```python
text = "You are so dumb and stupid!"
scores = model.classify(text)

print("Toxicity Scores:", scores)
```

**Example Output:**

```python
{
    "toxic": 0.85,
    "severe_toxic": 0.12,
    "obscene": 0.78,
    "threat": 0.05,
    "insult": 0.90,
    "identity_hate": 0.03
}
```

---

### ðŸ”¹ Get Overall Toxicity Score

```python
toxicity = model.predict(text)
print(f"Overall Toxicity Score: {toxicity:.4f}")
```

---

### ðŸ”¹ Classify Multiple Comments

```python
texts = [
    "I hate this!",
    "You're amazing!",
    "This is the worst thing ever!"
]

scores = model.predict_batch(texts)

for txt, score in zip(texts, scores):
    print(f"Text: {txt} --> Toxicity Score: {score:.4f}")
```

---

## ðŸ“„ License

This project is licensed under the MIT License.

```

```
