Metadata-Version: 2.4
Name: taoverse
Version: 2.0.2
Summary: A utilities library for model training subnets.
Project-URL: Homepage, https://github.com/macrocosm-os/taoverse
Project-URL: Issues, https://github.com/macrocosm-os/taoverse/issues
Author: Taoverse
License-File: LICENSE
Classifier: Intended Audience :: Developers
Classifier: License :: OSI Approved :: MIT License
Classifier: Operating System :: OS Independent
Classifier: Programming Language :: Python :: 3 :: Only
Classifier: Programming Language :: Python :: 3.9
Classifier: Programming Language :: Python :: 3.10
Classifier: Topic :: Scientific/Engineering
Classifier: Topic :: Scientific/Engineering :: Artificial Intelligence
Classifier: Topic :: Software Development
Classifier: Topic :: Software Development :: Libraries
Classifier: Topic :: Software Development :: Libraries :: Python Modules
Requires-Python: >=3.9
Requires-Dist: bittensor==9.0.0
Requires-Dist: colorama
Requires-Dist: huggingface-hub
Requires-Dist: jieba==0.42.1
Requires-Dist: librosa==0.10.2.post1
Requires-Dist: numpy
Requires-Dist: pypinyin==0.53.0
Requires-Dist: safetensors
Requires-Dist: torch
Requires-Dist: torchdiffeq==0.2.5
Requires-Dist: transformers
Requires-Dist: wandb
Requires-Dist: x-transformers==2.1.5
Description-Content-Type: text/markdown

# Taoverse Package

This is a package containing various python modules for use in bittensor subnets.

Currently it is primarily supporting the Macrocosmos SN 9 Pretraining and SN 37 Finetuning subnets but much of the code is generally useful and all are welcome to leverage it for their own work as well.

Find the latest published package here: https://pypi.org/project/taoverse/

Following is an overview of each module and some of the bigger pieces of code they include.

## Metagraph

The metagraph module contains code relating to metagraph operations.

- The `MetagraphSyncer` allows for easily refreshing a specified metagraph at a specified cadence. It will operate in asynchronously on the asyncio event loop and will notify any registered listeners after each sync.

- The `MinerIterator` provides a thread safe infinite iterator that safely handles adding new uids.

## Model

The model module contains code relating to large language models.

- The `ModelTracker` will track model metadata and evaluation history on a per hotkey basis. It is designed to keep state across restarts and has methods to save and load state.

- The `ModelUpdater` reads metadata from the chain, validates it, and checks if it is an update to a previous tracked hotkey.

- The model.competition module supports grouping models into distinct competitions.
  - The `CompetitionTracker` helps track weights at a per competition and per subnet level. Like the `ModelTracker` it is designed to keep state across restarts.
  - The `EpislonFunc` allows for computing epsilon with a custom function on a per competition basis. Both a `FixedEpsilon` and a `LinearDecay` implementation are included.

- The model.storage module supports storing and retrieving models and metadata.
  - For models there are included implementations for Hugging Face and the local disk.
  - For metadata there is an included implementation using the Bittensor chain.

## Utilities

The utilities module contains code found to be generically useful in the context of subnets.

- The `PerfMonitor` is a context manager that tracks the performance of a block of code by taking several samples
- In `utils.py` there are some helpers to `run_in_thread` or `run_in_subprocess`.
  - Running metagraph operations in a separate thread with a ttl can help avoid getting stuck on those actions.
  - Running model evaluation within a sub process can help ensure that the model is completely removed from the gpu afterwards.